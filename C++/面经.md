# C++

## 语法

### 定义和声明的区别？

**对于变量**：声明只是告诉编译器，有某个类型的变量将要被使用，但是并不会为其分配内存。而定义就会直接分配内存

**对于函数**：声明一般都在头文件里，告诉编译器存在这个函数。而定义一般都在源文件里，是函数的具体实现

- 声明仅仅只是告诉编译器某个类型的变量以及函数存在，不会分配具体的内存，定义会在定义的地方分配存储空间
- 相同的变量可以多处声明，但是只能在一处定义



### 初始化和赋值的区别

- 对于简单类型来说，没什么区别
- 对于类和复杂类型来说，初始化会调用拷贝构造函数，赋值会调用重载的赋值运算符



### 全局变量和局部变量

- 生命周期不同，全局变量随主程序的创建而创建，销毁而销毁，局部变量随某个函数或者循环创建以及销毁
- 位置不同，全局变量分配在全局数据区，各个程序的各个部分都能用到。而局部变量分配在 堆栈区，只能局部使用



### 形参和实参的区别

- 时间上来看，形参只有在函数别调用的时候，才会在堆栈区分配内存，并且只能在函数内部使用，函数结束就会销毁内存。而实参在函数调用的时候，必须有确定的值。
- 从数量及类型上看，实参和应参必须是一一对应的，包括数量，类型都应该严格一致
- 从数据流流向来看，只能把实参的值传递给形参，而不能把形参的值传递给实参，因此，函数调用的时候，形参的值会发生改变而实参不会。而如果是引用或者指针类型，那么形参改变而实参也会跟着变化



### 说说静态变量，全局变量，局部变量？







## 面向对象

## 智能指针

## 内存

### 谈谈C++的内存对齐？

[博客（地址按字节寻址，移动一个字节加一）](https://blog.csdn.net/weixin_48896613/article/details/127371045)

- 内存对齐：在 C 语言中，结构体是一种复合数据类型，它的构成元素可以是基本数据类型（int、long、float等），也可以是复合数据类型（数组、结构体等）的数据单元。为了让 CPU 能够对变量进行快速访问，变量的起始地址应该具有某种特性，即 "对齐"。例如 4 字节的 int 型，它的起始地址应该位于 4 字节的边界上，也就是能被 4 整除。这样就称之为自然对齐，于是 CPU 可以一次取出数据
- C++ 内存对齐：用于三种数据类型：struct / class / union，对齐原则有四个
  1. 数据成员对齐规则： struct 或者 union 的数据成员，第一个数据成员放在 offset 为 0 的地方，以后每个数据成员存储的起始位置都要从该成员大小或者成员的子成员大小的整数倍开始
  2. struct 作为成员：如果一个结构里有某些结构体成员，则结构体成员要从其内部 “最宽基本类型成员” 的整数倍地址开始存储（struct A 里面有成员 struct B，B里的成员有 char，int， double等元素，那 B 应该从 8 的整数倍开始存储）
  3. 收尾工作：结构体的大小，也就是 sizeof 的结果，必须是其内部最大成员的 “最宽基本类型成员” 的整数倍。不足的要补齐
  4. sizeof(union) 以结构里面 size 最大的元素为 union 的 size，因为在某一时刻，union 只有一个成员真正的存储在该地址



### 为什么C++没有垃圾回收机制？

垃圾收集器：是一种动态存储分配器，它自动释放程序不再需要的已分配的块，这些块也被成为垃圾。在程序员看来，垃圾就是不再被引用的对象，自动回收垃圾的过程则称为垃圾收集。

- 首先，实现一个垃圾回收器会带来额外的空间和事件开销，需要开辟一定的空间保存指针的引用计数和对他们进行标记 mark，然后需要单独开辟一个线程在空闲的时候进行 free 操作
- 垃圾回收会使得 C++ 不适合进行很多底层操作



### 什么是内存泄漏？

内存泄露：简单的说就是申请了一块内存地址，但是用完之后没有释放掉

- new 和 malloc 申请资源使用后，没有使用 delete 和 free 释放
- 子类继承父类时，父类析构函数不是虚函数
- Windows 句柄资源使用后没有释放

后果：只发生一次小的内存泄漏可能不被注意，但泄露大量的内存程序将会出现各种症状；性能下降到内存逐渐用完，导致另一个程序失败

如何避免：

- 使用内存分配的函数，一旦使用完毕，要记得使用其对应的函数释放掉。有 new 就有 delete，有 malloc 就有 free，对象数组一定要用 delete []
- 一定要把基类的析构函数声明为 虚函数
- 使用智能指针



### C++ 的内存分区？

1. 栈区：存放函数的参数值和局部变量的值，由编译器自动分配和释放
2. 堆区：由程序员申请后使用，需要手动释放否则会造成内存泄漏。如果程序员没有手动释放，那么程序结束可能由操作系统回收
3. 全局/静态 存储区：存放全局变量和静态变量，初始化的全局变量和静态变量放在一块，未初始化的放在另一块
4. 常量区：存放常量字符串、常量变量等。程序启动时自动分配，程序结束时系统释放。
5. 程序代码区：存放程序的二进制代码，内存由系统管理



### 堆和栈的区别？

|                |             堆                                               |                    栈                                          |
| ---------------- | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **管理方式**     | 堆中资源由程序员控制（容易产生memory leak）                  | 栈资源由编译器自动管理，无需手工控制                         |
| **内存管理机制** | 系统有一个记录空闲内存地址的链表，当系统收到程序申请时，遍历该链表，寻找第一个空间大于申请空间的堆结点，删 除空闲结点链表中的该结点，并将该结点空间分配给程序（大多数系统会在这块内存空间首地址记录本次分配的大小，这样delete才能正确释放本内存空间，另外系统会将多余的部分重新放入空闲链表中） | 只要栈的剩余空间大于所申请空间，系统为程序提供内存，否则报异常提示栈溢出。（这一块理解一下链表和队列的区别，不连续空间和连续空间的区别，应该就比较好理解这两种机制的区别了） |
| **空间大小**     | 堆向高地址扩展，是不连续的内存区域，大小可以灵活调整。 堆是不连续的内存区域（因为系统是用链表来存储空闲内存地址，自然不是连续的），堆大小受限于计算机系统中有效的虚拟内存（32bit 系统理论上是4G），所以堆的空间比较灵活，比较大 | 栈顶和栈底是之前预设好的，栈是向栈底扩展，大小固定，可以通过ulimit -a查看，由ulimit -s修改。 栈是一块连续的内存区域，大小是操作系统预定好的，windows下栈大小是2M（也有是1M，在 编译时确定，VC中可设置） |
| **碎片问题**     | 对于堆，频繁的new/delete会造成大量碎片，使程序效率降低       | 对于栈，它是有点类似于数据结构上的一个先进后出的栈，进出一一对应，不会产生碎片。（看到这里我突然明白了为什么面试官在问我堆和栈的区别之前先问了我栈和队列的区别） |
| **生长方向**     | 堆向上，向高地址方向增长。                                   | 栈向下，向低地址方向增长。                                   |
| **分配方式**     | 堆都是动态分配（没有静态分配的堆）                           | 栈有静态分配和动态分配，静态分配由编译器完成（如局部变量分配），动态分配由alloca函数分配，但栈的动态分配的资源由编译器进行释放，无需程序员实现。 |
| **分配效率**     | 堆的操作是由C/C++函数库提供的，在分配堆内存的时候需要一定的算法寻找合适大小的内存。并且获取堆的内容需要两次访问，第一次访问指针，第二次根据指针保存的地址访问内存，因此堆比较慢。 | 栈是其系统提供的数据结构，计算机在底层对栈提供支持，分配专门 寄存器存放栈地址，栈操作有专门指令。 |



### segment fault是什么错误？

通常是以下几个原因：

- 内存访问越界
- 多线程程序使用了线程不安全的函数
- 非法指针
- 堆栈溢出



### new/delete、malloc/free

- new/delete 是 C++ 操作符关键字，需要编译器支持， malloc 是 C/C++  标准库函数，需要头文件支持
- 使用 new 申请内存时，不用指定内存块的大小， 使用 malloc 申请内存时，需要显式的指出需要分配的内存大小
- new 分配失败时，会抛出 bac_alloc 异常，而 malloc 失败时会返回 NULL
- new 分配成功时会返回对象类型的指针，但是 malloc 成功时会返回 void，需要强转成 需要的类型
- new/delete 会调用对象的构造函数 和 析构函数，而 malloc/free 不会调用



### 被free回收的内存会立即返还给操作系统吗？

不是的，被 free 的内存会使用双链表保存起来，当用户下一次申请内存的时候，会尝试在链表中寻找合适的内存，这样就避免了频繁的系统调用



### C++中有几种 new

plain new，nothrow new, placement new



### delete p, delete [] p 的区别？

- delete 只会调用一次析构函数
- delete [] p 会调用数组中每个元素的析构函数
- delete [] 的时候，按数组中元素的逆序销毁



### 内存池？

内存池（Memory Pool） 是一种**内存分配**方式。通常我们习惯直接使用new、malloc 等申请内存，这样做的缺点在于：由于所申请内存块的大小不定，当频繁使用时会造成大量的内存碎片并进而降低性能。内存池则是在真正使用内存之前，先申请分配一定数量的、大小相等(一般情况下)的内存块留作备用。当有新的内存需求时，就从内存池中分出一部分内存块， 若内存块不够再继续申请新的内存。这样做的一个显著优点是尽量避免了内存碎片，使得内存分配效率得到提升。



### C++中类的数据成员和成员函数内存分布情况？

- 类的大小：
  - **非静态成员**的数据类型大小之和
  - 边缘对齐优化加入 padding：内存对齐另外分配的空间大小，类内的数据也是需要进行内存对齐操作的
  - 有虚函数的话，会在类对象插入vptr指针，加上指针大小
- 静态成员不占据类的空间，成员函数也不占据类的空间大小
- 当该类是某类的派生类，那么派生类继承的基类部分的数据成员也会存在在派生类中的空间中，也会对派生类进行扩展
- 一个类对象的地址就是类所包含的这一片内存空间的首地址，这个首地址也就对应具体某一个成员变量的地址
- 对象的大小和对象中数据成员的大小是一致的，成员函数不占用对象的内存。这是因为所有的函数都是存放在代码区的，不管是全局函数，还是成员函数。
- 静态成员函数的存放问题：静态成员函数与一般成员函数的唯一区别就是没有this指针，因此不能访问非静态数据成员。所有函数都存放在代码区，静态函数也不例外。所有有人一看到 static 这个单词就主观的认为是存放在全局数据区，那是不对的。



### 深拷贝和浅拷贝？

浅拷贝：浅拷贝只是拷贝一个指针，并没有开辟新的内存空间，拷贝的指针和原来的指针指向同一块地址，如果原来的指针所指向的资源释放了，那么再释放浅拷贝的指针的资源就会出错

在计算机中开辟了一块新的内存地址用于存放复制的对象。

深拷贝不仅拷贝值，还开辟出一块新的空间用来存放新的值，即使原先的对象被析构掉，释放内存了也不会影响到深拷贝得到的值。在自己实现拷贝赋值的时候，如果有指针变量的话是需要自己实现深拷贝的。



### 对象复用和零拷贝？

**对象复用**的本质是一种设计模式：Flyweight享元模式

通过把对象存储到“对象池”中实现对象的重复利用，可以避免多次创建重复对象的开销，节省系统资源

**零拷贝**：是一种避免 CPU 把数据从一块存储拷贝到另外一块存储的技术，可以减少数据拷贝和共享总线操作的次数

在 C++ 中，vector 的成员函数 **emplace_back** 很好的体现了零拷贝技术，跟 **push_back** 一样可以把元素车入到容器的尾部，区别在于：**使用push_back()函数需要调用拷贝构造函数和转移构造函数，而使用emplace_back()插入的元素原地构造，不需要触发拷贝构造和转移构造**，效率更高。



### 类如何实现只能静态分配？和只能动态分配？

建立类的对象有两种方式：

① 静态建立：静态建立一个类对象，就是由编译器为对象在栈空间中分配内存

② 动态建立：A *p = new A(); 动态建立一个类对象，就是使用new运算符为对象在堆空间中分配内存。这个过程分为两步，第一步执行operator new()函数，在堆中搜索一块内存并进行分配；第二步调用类构造函数构造对象

**只能动态分配**

类对象只能通过new运算符建立在堆空间中，不能静态分配，即不能直接调用类的构造函数

1.  private 构造函数（不可行），没法调用 构造函数
2. private 析构函数（无法实现继承），如果类的析构函数在类外部无法访问，则编译器拒绝在栈空间上为类对象分配内存。因此，可以将析构函数设为private，这样就无法在栈上建立类对象了。但是当作为基类让子类继承的时候，需要把析构函数设置为 virtual 函数，然后子类重写虚析构函数，但如果是 private 的话，子类是无法访问的。
3. 构造、析构函数设为 protected。将析构函数设为protected，类外无法访问protected成员，但是子类可以访问。

**只能静态分配**

只有使用 new 运算法，才会在堆上动态创建对象，于是，限制 new 运算符的使用即可。可以把 new 运算符设置为 私有



### 函数调用的过程

1. 第一个进栈的是（主函数中的） 调用处的下一条指令（即函数调用语句的下一条可执行语句） 的地址；

2. 然后是函数的各个参数，而在大多数C/C++编译器中，在函数调用的过程中，函数的参数是 由右向左入栈的；

3. 然后是函数内部的 局部变量（注意static变量是不入栈的）；

在函数调用结束（函数运行结束）后

1. 局部变量最先出栈

2. 然后是参数

3. 最后栈顶指针指向最开始存的指令地址，程序由该点继续运行。



### 谈谈智能指针?

智能指针是一个类，用来存储指向动态分配对象的指针，负责自动释放动态分配的对象，防止堆内存泄漏。动态分配的资源，交给一个类对象去管理，当类对象声明周期结束时，自动调用析构函数释放资源





## 容器

## 多线程

## 模板

## 编译和链接

### C++从代码到可执行二进制程序的流程

C++从源文件到可执行文件一共有四个步骤，**预编译、编译、汇编、链接**

**预编译：**将头文件、宏进行展开

1. 删除所有的 #define，并展开所有的宏定义
2. 删除所有的条件预编译指令，#if，#ifdef
3. 处理所有的 include 预编译指令，把被包含的文件插入到预编译指令的位置
4. 过滤所有的注释
5. 添加行号和文件名标识

**编译：**把预编译生成的 .i 或者 .ii 文件，进行一系列的 **词法分析、语法分析、语义分析以及优化** 后，生成相应的汇编代码文件

1. 词法分析：利用类似于“有限状态机”的算法，将源代码程序输入到扫描机中，将其中的字符序列分 割成一系列的记号。
2. 语法分析：语法分析器对由扫描器产生的记号，进行语法分析，产生语法树。由语法分析器输出的 语法树是一种以表达式为节点的树。
3. 语义分析：语法分析器只是完成了对表达式语法层面的分析，语义分析器则对表达式是否有意义进 行判断，其分析的语义是静态语义——在编译期能分期的语义，相对应的动态语义是在运行期才能确定 的语义。
4. 优化：源代码级别的一个优化过程。
5. 目标代码生成：由代码生成器将中间代码转换成目标机器代码，生成一系列的代码序列——汇编语言 表示。
6. 目标代码优化：目标代码优化器对上述的目标机器代码进行优化：寻找合适的寻址方式、使用位移 来替代乘法运算、删除多余的指令等。 

**汇编：**把编译生成的汇编代码转换成机器可以执行的指令。汇编之后，产生目标文件 xxx.o(Windows 下)、xxx.obj(Linux下)



**链接：**把不同源文件产生的目标文件进行链接，从而形成一个可执行程序。链接分为静态链接和动态链接

静态链接：在链接的时候就把需要调用的函数链接到了可执行文件中。生成的静态链接库，Windows下为 .lib，Linux 下为 .a

- 优点是，运行速度快，不依赖动态链接库
- 缺点是，空间浪费，更新代码困难

动态链接：在链接的时候，没有把需要的函数链接到可执行文件，而是在执行的过程中，再根据可执行文件中函数的重定位信息去找要链接的函数。生成的动态链接库在 Windows 下是 .dll，在 Linux 下是 .so

- 优点是，更新方便，只需要替换原来的目标文件。动态编译，加快了编译速度，节省了系统资源
- 缺点是，性能损耗，每次执行都要链接。如果计算机上没有安装相应的动态库，那可执行文件就不能运行



### 什么是 GCC，工作原理是什么？？

gcc 全称是 GNU **Compiler Collection** ，它是能够编译多种语言的编译器





### 写个函数在 main 函数前面执行？

全局变量的构造函数会在 main 函数之前执行

```C++
class App{
public:
    App(){
        cout << "First" << endl;
    }
};

App a;

int main(){
    cout << "Second" << endl;
    return 0;
}
```



# 操作系统

## 内存管理

![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230330143244.png)

![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230330143328.png)

- 代码段：包括可执行的二进制代码
- 数据段：包括已初始化的全局变量和静态变量
- BSS段：包括未初始化的全局变量和静态变量
- 堆：包括动态分配的内存，从低地址向上增长
- 文件映射段：包括动态库，共享内存等
- 栈：包括局部变量和函数调用的上下文等





###  malloc 是如何分配内存的？

malloc 申请内存的时候，会有两种方式向操作系统申请内存

1. 通过 brk() 系统调用从堆分配内存
2. 通过 mmap() 系统调用在文件映射区分配内存

一般情况下，当申请的内存小于 128KB，则通过 brk() 申请内存，大于 128KB，通过 mmap() 申请内存



###  malloc() 分配的是物理内存吗？

不是的，malloc 首先分配的是虚拟内存

如果分配的虚拟内存没有被访问的情况下，是不会映射到物理内存的

当分配的虚拟内存被访问时，通过查找页表，发现虚拟内存对应的页不在物理内存中，就会发生缺页中断，然后操作系统才会建立虚拟内存和物理内存之间的映射关系



### malloc(1) 会分配多大的虚拟内存？

malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。



### free 释放内存，会归还给操作系统吗？

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。





### 为什么不全部使用 mmap 来分配内存？

- 向操作系统申请内存需要通过系统调用，执行系统调用需要切换到内核态，执行完后再切换回用户态，开销比较大

- 因为通过 mmap() 方式申请的内存，free 之后会立刻返回给操作系统，那么每次第一次访问的时候，都会缺页中断

使用 brk() 在堆空间申请内存可以预分配更大的内存作为内存池，当内存释放的时候，会缓存在内存池里。下次申请的话，直接从内存池取出，可能当前的虚拟内存之前已经和物理内存建立映射关系，减少缺页中断次数



### 既然 brk 那么牛逼，为什么不全部使用 brk 来分配？

free 不回收的话，会产生较多的内部碎片





###  free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？

malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节

多出来的 16 字节就是保存了该内存块的描述信息，比如有该内存块的大小。



### 虚拟内存有什么作用？

1. 虚拟内存可以使进程的运行内存超过物理内存，因为程序运行符合局部性原理，CPU访问内存时有明显的重复性，对于那些没有经常被用到的内存，可以把它换出物理内存之外，例如硬盘的 Swap 区
2. 由于每个进程都有自己的页表，所以每个进程之间的虚拟内存是相互独立的，进程没法访问别的进程的页表，解决了多进程之间地址冲突的问题
3. 页表里页表项除了物理地址之外，还有一些标记属性的比特，控制一个页的读写权限，改页是否存在等。在访问内存的时候，提供了更好的安全性





### 内存回收

如果没有空闲的物理内存，那么内核就会开始进行**回收内存**的工作，回收的方式主要是两种：直接内存回收和后台内存回收。

- **后台内存回收**（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程**异步**的，不会阻塞进程的执行。
- **直接内存回收**（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是**同步**的，会阻塞进程的执行。

如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——**触发 OOM （Out of Memory）机制**。

OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置



### 哪些内存可以被回收？

可被回收的内存类型有文件页和匿名页：

- 文件页的回收：对于干净页是直接释放内存，这个操作不会影响性能，而对于脏页会先写回到磁盘再释放内存，这个操作会发生磁盘 I/O 的，这个操作是会影响系统性能的。
- 匿名页的回收：如果开启了 Swap 机制，那么 Swap 机制会将不常访问的匿名页换出到磁盘中，下次访问时，再从磁盘换入到内存中，这个操作是会影响系统性能的。



### 针对回收内存导致的性能影响，常见的解决方式。

- 设置 /proc/sys/vm/swappiness，调整文件页和匿名页的回收倾向，尽量倾向于回收文件页；
- 设置 /proc/sys/vm/min_free_kbytes，调整 kswapd 内核线程异步回收内存的时机；









## 进程与线程

### 系统并发和并行，分得清吗？

并行指的是同时运行多个程序，需要硬件的支持，包括多核处理器，多流水线，分布式计算系统等

并发是每个程序执行一段时间，宏观上看相当于是并行执行



### 进程、线程和协程的区别和联系?

|          | 进程                                                         | 线程                                               | 协程                                                         |
| -------- | ------------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ |
| 定义     | 资源分配和拥有的基本单位                                     | 程序执行的基本单位                                 | 用户态的轻量级线程，线程内部调度的基本单位                   |
| 切换情况 | 进程CPU环境(栈、寄存器、页表和文件句柄等)的保存以及新调度的进程CPU环境的设置 | 保存和设置程序计数器、少量寄存器和栈的内容         | 先将寄存器上下文和栈保存，等切换回来的时候再进行恢复         |
| 切换者   | 操作系统                                                     | 操作系统                                           | 用户                                                         |
| 切换过程 | 用户态->内核态->用户态                                       | 用户态->内核态->用户态                             | 用户态(没有陷入内核)                                         |
| 调用栈   | 内核栈                                                       | 内核栈                                             | 用户栈                                                       |
| 拥有资源 | CPU资源、内存资源、文件资源和句柄等                          | 程序计数器、寄存器、栈和状态字                     | 拥有自己的寄存器上下文和栈                                   |
| 并发性   | 不同进程之间切换实现并发，各自占有CPU实现并行                | 一个进程内部的多个线程并发执行                     | 同一时间只能执行一个协程，而其他协程处于休眠状态，适合对任务进行分时处理 |
| 系统开销 | 切换虚拟地址空间，切换内核栈和硬件上下文，CPU高速缓存失效、页表切换，开销很大 | 切换时只需保存和设置少量寄存器内容，因此开销很小   | 直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快 |
| 通信方面 | 进程间通信需要借助操作系统                                   | 线程间可以直接读写进程数据段(如全局变量)来进行通信 | 共享内存、消息队列                                           |

1、进程是资源调度的基本单位，运行一个可执行程序会创建一个或多个进程，进程就是运行起来的可执行程序

2、线程是程序执行的基本单位，是轻量级的进程。每个进程中都有唯一的主线程，且只能有一个，主线程和进程是相互依存的关系，主线程结束进程也会结束。

3、协程是用户态的轻量级线程，线程内部调度的基本单位



### **线程和进程的区别？**

**调度：**进程是资源分配的基本单位，而线程是CPU调度的基本单位

**并发性**：多个进程可以并发，一个进程内多个线程可以并发

**拥有资源**：进程拥有完整的资源，而线程共享进程内的代码段、数据段、打开的文件等资源，拥有自己独立的寄存器和栈

**系统开销**：进程的创建和销毁需要处理task_struct结构，而线程创建销毁只需要处理 PC 值，通用寄存器值，线程栈等，开销较小，速度快



### 一个进程可以创建多少线程，和什么有关？

- **进程的虚拟内存空间上限**，因为创建一个线程，操作系统需要为其分配一个栈空间，如果线程数量越多，所需的栈空间就要越大，那么虚拟内存就会占用的越多
- **系统参数限制**，虽然 Linux 并没有内核参数来控制单个进程创建的最大线程个数，但是有系统级别的参数来控制整个系统的最大线程个数

过多的线程将会导致大量的时间浪费在线程切换上，给程序运行效率带来负面影响，无用线程要及时销毁



### 用户线程和内核线程

**用户线程**：在用户空间实现的线程，不是由内核管理的线程，由用户态的线程库来完成线程的管理；那么**线程控制块（\*Thread Control Block, TCB\*）** 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。所以，**用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。**

优点：

- 每个进程都需要有它私有的线程控制块（TCB）列表，用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统；
- 用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快

缺点：

- 由于操作系统不参与线程的调度，如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行了
- 当一个线程开始运行后，除非它主动地交出 CPU 的使用权，否则它所在的进程当中的其他线程无法运行，因为用户态的线程没法打断当前运行中的线程，它没有这个特权，只有操作系统才有，但是用户线程不是由操作系统管理的
- 由于时间片分配给进程，在多线程执行时，每个线程得到的时间片较少，执行会比较慢



**内核线程**：在内核中实现的线程，是由内核管理的线程；**线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。**

优点：

- 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行
- 分配给线程，多线程的进程获得更多的 CPU 运行时间

缺点：

- 在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如 PCB 和 TCB
- 线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大



**轻量级进程**：是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持，而且 LWP 是由内核管理并像普通进程一样被调度



### 进程调度算法你了解多少？

非抢占式：只要得到 CPU ，一直执行完之后才执行别的作业

抢占式：通过调度算法，如果被选中了，不断 CPU 是否在执行别的作业，直接抢占

1. 先来先服务（FCFS）：非抢占式调度算法，按照请求的顺序进行调度。有利于长作业，但不利于短作业

2. 短作业优先（SJF）：非抢占式的调度算法，按估计运行时间最短的顺序进行调度。有利于短作业，但不利于长作业

3. 高响应比优先调度算法：**每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**，「响应比优先级」的计算公式：`优先权=（等待时间 + 要求服务时间）/ 要求服务时间`。因为不知道一个进程要求服务的时间，所以，高响应比优先调度算法是「理想型」的调度算法，现实中是实现不了的

4. 时间片轮转调度算法：将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。

5. 优先级调度：为每个进程分配一个优先级，按优先级进行调度。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。

6. 多级反馈队列：

   - 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；

   - 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；

   - 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；

     ![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230320230734.png)








### Linux下进程间通信方式？

- 管道：管道是一种半双工的通信方式，数据只能单向流动
  - 匿名管道：没有名称，就是 Linux 里面的 `|` ，只能在具有血缘关系的进程之间使用
  - 命名管道：可以不用在父子进程之间的管道进行通信
- 消息队列：是消息的链表，存放在内核中并由消息队列标识符标识，解决了管道传递的信息少，只能承载无格式数据流的缺点
- 共享内存：映射一段能够被几个进程访问的内存，针对进程间通信效率低而设计的，往往和信号量配合来使用
- 信号：用来通知接受进程某个事件的发生，例如 `Ctrl + C ` 就是 `SIGINT` 信号，`Ctrl + Z` 就是 `SIGTSTP` 信号。
- 信号量：它是一个计数器，可以用来控制多个进程对共享资源的访问。经常作为一种锁机制，实现进程、线程的临界区的同步和互斥访问
- Socket：可以用于不同机器间进程的通信，也可以用于本地进程通信



### Linux下同步机制？

- POSIX信号量：可用于进程同步，也可用于线程同步
- POSIX互斥锁 + 条件变量：只能用于线程同步



### 死锁是什么？

死锁是两个或以上的线程都在等待对方释放资源的过程，死锁会导致程序卡死



### 死锁产生的条件是什么？

1. **互斥条件**：进程对所需求的资源具有排他性，若有其他进程请求该资源，请求进程只能等待。
2. **不剥夺条件**：进程在所获得的资源未释放前，不能被其他进程强行夺走，只能自己释放。
3. **请求和保持条件**：进程当前所拥有的资源在进程请求其他新资源时，由该进程继续占有。
4. **循环等待条件**：存在一种进程资源循环等待链，链中每个进程已获得的资源同时被链中下一个进程所请求。



### 死锁的处理方法？

- 鸵鸟策略：装作死锁没有发生，忽略问题
- 死锁检测和死锁恢复：不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复
  - 利用抢占恢复
  - 利用回滚恢复
  - 通过杀死进程恢复
- 死锁预防：在程序运行之前预防发生死锁，破坏上述四个条件
- 死锁避免：在程序运行时避免发生死锁，银行家算法



### 几种典型的锁？

- 互斥锁：一次只能一个线程拥有互斥锁，其他线程只有等待，会进入阻塞状态，等待内核唤醒
- 自旋锁：当线程无法获得锁的时候，不会放弃CPU，而是会一直等待
- 读写锁：分为读锁和写锁，当读取共享资源的时候使用读锁，写共享资源的时候使用写锁。多个读者可以同时读，但是只允许一个写者写

上面都属于悲观锁，就是在访问共享资源之前，默认会修改，先加锁

- 乐观锁：在访问共享资源之前不加锁，而是先访问，修改完之后，再判断这段时间有没有发生冲突，如果没有冲突，操作完成。如果发生冲突，则操作失败







## 中断和异常

### 外中断和异常有什么区别？

外中断：由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理完成。此外还有时钟中断，控制台中断等

异常：是由 CPU 执行指令的内部事件引起，例如非法操作码，地址越界，算术溢出等



### 软中断？

为了避免 CPU 由于中断处理程序时间过长，从而影响正常程序的调度，把中断处理程序分为上下两个部分，

- 上半部：对应硬中断，由硬件触发中断，用来快速处理中断
- 下半部：对应软中断，由内核触发中断，用来异步处理上半部未完成的工作





















# 计算机网络

### 为什么需要 TCP 协议？ TCP 工作在哪一层？

因为 IP 层是不可靠的，并不保证网络包的按序交付以及网络包中数据的完整性

因此，如果需要保证交付数据的可靠性，就需要上层的协议 TCP 协议来保证。

该协议位于传输层，可以保证接受网络包是无损坏以及按照顺序的



### 什么是 TCP ？

TCP 是面向连接的，可靠的，基于字节流的传输层通信协议



# 数据库



# Webserver

### 为什么要做这个项目？

当刚学完C++语法的时候，看完 C++ primer 与 TCP/IP 网络编程 以及 游双的 Linux 高性能网络编程，想要通过项目来提升编程能力，其中 webserver 就是 C++ 中很经典的项目

该项目使用了网络编程技术，用到了很多比较经典的技术方法，可以快速的帮助我实践网络编程，搭建自己的服务器



### 介绍下你的项目？

该项目使用了线程池技术高并发的快速处理请求

使用 非阻塞 socket 和 epoll 技术实现对连接客户端的请求监听（连接，EPOLLIN，EPOLLOUT等），触发方式 ET 和 LT 都实现了

事件处理模式使用了Reactor 和 模拟 Proactor 模式都实现

在解析请求方面，使用了状态机解析HTTP报文请求，支持解析 GET 和简单的 POST 请求

实现了 web 端的用户注册，登录，可以请求服务器的图片和视频

使用的是同步日志记录服务器的运行状态

经过 Webbench 可以实现上万并发请求



### 线程池的成员变量有哪些？手写一下？

线程池的数量 m_thread_num, 线程池队列 m_threads, 请求的最大数量 m_max_requests, 请求队列 m_workqueue, 互斥锁 m_mutex, 信号量 m_sem



### 线程的同步机制有哪些？

有**4种**，临界区，互斥锁，信号量，条件变量

首先是临界区，通过多线程的互串行访问公共资源或一段代码，速度快，适合控制数据访问。

然后是互斥锁，当向线程池中的请求队列中加入请求或者拿出请求的时候，需要对其进行上锁，这样防止两个线程同时操作请求队列造成错误

然后是信号量，当有请求到来时，信号量加一，表示目前可以获得的请求多了一个，然后也会有线程请求信号量，拿到请求后，会把信号量减一

最后是条件变量，提供了线程间的一种通知机制，当某个共享数据达到某个值的时候，会以广播的方式唤醒等待的线程



### 线程池中的工作线程是一直等待吗？

线程池中的工作线程是处于一直阻塞等待的模式下的。

如果请求队列中目前没有请求，那么线程会 sem.wait() 等待新的请求进入到请求队列



### 你的线程池工作线程处理完一个任务后的状态是什么？

分**两种**情况考虑

（1） 当处理完任务后如果请求队列为空时，则这个线程重新回到阻塞等待的状态

（2） 当处理完任务后如果请求队列不为空时，那么这个线程将处于与其他线程竞争资源的状态，谁获得锁谁就获得了处理事件的资格。



### 如果同时1000个客户端进行访问请求，线程数不多，怎么能及时响应处理每一个呢？

本项目中是通过对子线程循环调用来解决高并发的问题的

在创建线程的时候，使用 `pthread_detach` 把线程分离，这样就不需要手动进行回收了。但是当子线程任务完成之后，它的资源会自动被系统回收。

这样的话，线程池就只能处理例如 8，10 个请求，这是不合理的。

于是，我们让线程进行 while 循环，即使它执行完了自己的任务，也不会被回收资源，而是一直循环等待的请求队列的请求，有的话，就获得请求，继续执行新的请求，没有的话，就阻塞等待。直到最后线程池被销毁



### 如果一个客户请求需要占用线程很久的时间，会不会影响接下来的客户请求呢，有什么好的策略呢?

会影响，因为线程池中的线程数量是有限的，如果用户长时间占用线程，会影响处理请求的效率。

可以为线程处理请求设置超时时间，超过设置的时间，就会通知线程，如果这个请求还在占用线程，就断开连接



### 简单说一下服务器使用的并发模型？

利用IO复用技术Epoll与线程池实现多线程的Reactor 和 模拟 Proactor 高并发模型

总的来说，使用主线程来监听事件，线程池中的多线程来处理请求

当使用 Reactor 处理模式的时候，主线程监只负责监听，当有客户端的请求，会把请求封装放进线程池中的请求队列，等待工作线程读取数据并且处理请求，最后处理完成之后， 触发 EPOLLOUT 事件，主线程检测到事件后，调用工作线程去进行写操作

当使用模拟 Proactor 处理模式的时候，主线程不仅负责监听事件，还同时读取数据和写数据。当有客户端请求的时候，会读出请求，之后再插入到线程池的请求队列。当工作线程处理完之后，会注册 EPOLLOUT 事件，由主线程负责写数据。



### 为什么 ET 要设置非阻塞 I/O？

当触发可读事件时，调用 read() 函数等，如果没有数据的话。阻塞 I/O 会等待直到有数据，非阻塞 I/O 会直接返回，通过 errno 判断

- 当是 LT 模式的时候，此时触发读事件，调用 read() 函数，此时有数据的话，拷贝到缓冲区。注意此时，如果一次读不完的话，因为没有 while 循环，所以会直接返回，但是，因为数据还没读完，所以会继续触发事件，继续调用 read () 函数。当数据全部读完了，那么也就不会触发读事件了。
- ok，如果设置为 阻塞 I/O ，当最后一次读完的时候，就不会触发读事件了，因此不会调用 read() ，不会阻塞
- 当是 ET 模式的时候，触发读事件，调用 read() 函数，因为需要一次读完，所以使用 while 循环。当读最后一次读完了所有的数据的时候，因为有 while 循环，仍然会调用 read() 函数，那么此时没有数据需要读。
- 如果设置的是 阻塞 I/O ，因为没有数据，会一直阻塞在 read() 那里，一直到有数据才可以继续进行，没办法出 while 循环。设置为非阻塞 I/O 的话，当没有数据的时候，会立即返回，可以根据 errno 判断是不是出错还是正常退出。

> 还有一种情况，就是当设置了 ET 模式下，如果读缓冲区太小，来不及读取数据的时候（**似乎不会出现这种情况**），同样会触发 errno 为 EAGAIN， EWOULDBLOCK，那么此时 break 之后，需要再次注册该 读事件，因为本次并没有读取完， ET 模式下并不会再次触发读事件，所以需要手动触发读事件 modfd 修改 EPOLLIN





### http连接请求处理

首先当浏览器发送HTTP请求过来的时候，主线程读取完成之后会把数据读取到 用户数组的 HTTP 对象的缓冲区中，然后把该对象插入到线程池中的任务队列，由线程池中的工作线程执行报文的解析，当解析完成之后，会触发 EPOLLOUT 事件，然后由主线程调用 HTTP 对象的写函数把数据从对象缓冲区写入到 Socket 的缓冲区中，发送给浏览器



### http报文解析处理流程

主要由两个函数，process_read() 和 process_write() ，一个用来解析 HTTP 报文，一个根据解析的结果生成相应报文

使用了主从状态机的转换来完成 HTTP 报文的解析，

首先，**主状态机**分为三个状态，分别代表当前在解析报文的哪个部分，包括解析请求行、请求头、请求体。**从状态机**是对一行读取的几种状态，包括，读取的行是完整的（LINE_OK）、读取的行是不完整的（LINE_OPEN）、读取的行是错误的（LINE_BAD）。从状态机读取一行，主状态机对一行进行解析，主状态机调用从状态机，从状态机驱动主状态机

从状态机从读缓冲区中进行读取，遇到 \r\n 就把它置为 \0 \0, 并且更新 m_checked_idx, 表示目前已经检查到的位置。主状态机初始状态是 CHECK_STATE_REQUESTLINE 表示正在解析请求行， 主状态机拿出已经处理过的一行数据，根据自己的状态，决定调用相应的函数对该行进行解析，如果解析成功就继续读取下一行，推动从状态机读取。最后，当读完请求之后，调用 do_request 对请求的内容进行判断，是否合法。最后，根据报文解析返回的状态码，调用 process_write() 函数生成响应报文。



### **http响应报文处理流程**

当报文解析完成之后，调用 process_write() 函数，首先根据返回的 HTTP 状态码判断，如果不是正常请求或者请求的文件没全新啊等，都是构建响应行，响应头以及空行和响应体

最后注册 EPOLLOUT 事件



### 用了状态机啊，为什么要用状态机？

传统的应用程序的执行流程基本是按照顺序执行的，遵循事先设定的逻辑，从头到尾执行。如果想在不同的状态下实现代码跳转，就需要破坏一些代码，这样就会造成代码逻辑混乱。所以必须采取不同的技术来处理这些情况。

**能够处理任何顺序的事件，并能提供有意义的响应--即使这些事件发生的顺序和预计的不同。**

有限状态机就是为了满足这一方面的要求而设计的，每个状态都有一系列的转移，每个转移和另外一个状态有关，当输入进来，如果和当前状态的某个转移匹配，机器转换为所指的状态，然后执行相应的代码。



### 状态机的转移图画一下？

![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230317202552.png)



### https协议为什么安全？

https = http + TLS/SSL

TLS/SSL 协议位于应用层和 TCP 层之间，构建在 TCP 之上，由 TCP 协议保证数据传输的可靠性，任何数据到达 TCP 之前，都会经过 TLS/SSL 协议处理

之前的 http 协议访问网站时，都是明文的，包括密码，账号交易等机密信息，很容易会被泄露、窃取、篡改。安装 SSL 证书之后，使用 https 协议访问网站，可以激活用户浏览器到服务器的之间的 SSL 加密通道，实现高强度的双向加密传输，防止数据泄露、被篡改。



### https的ssl连接过程

- 客户端提交 http 请求
- 服务器相应 客户，并把证书发给客户端
- 客户端验证证书公钥的有效性
- 有效后，会生成一个会话密钥
- 用证书的公钥加密这个 会话密钥后，发送给服务器端
- 服务器收到后，用相对应的私钥解密，获得会话密钥
- 之后客户端与服务器端利用这个会话密钥加密要传输的数据进行通信



### GET和POST的区别

参数位置：GET是在URL的query中，POST一般是在请求体中，也可以在query中

参数大小：GET受限于浏览器 URL 的大小，POST是 1G

服务器数据接收：GET是接收一次，POST根据数据的大小，可以分成多次接收

适用场景：GET是从服务端获取数据，不做增删改查，POST是向服务器提交数据

安全性：GET因为是在URL中，安全性低，POST相对于GET，安全性更高一点





















### 什么是阻塞和非阻塞以及同步和异步？

对于一次 I/O 来说，有数据就绪阶段以及数据读写两个阶段。

对于数据就绪阶段，根据系统 I/O 操作的就绪状态来区分阻塞和非阻塞。

- 阻塞 I/O是指，当用户线程发起 I/O 请求时，如果数据还未准备就绪，就会**阻塞**当前线程，让出 CPU
- 非阻塞 I/O 是指，当用户线程发起 I/O 请求时，如果数据还未准备就绪，也**不会阻塞**当前线程，可以继续执行后续的任务

对于数据读写阶段，根据系统 I/O 响应方式不同来区分同步和异步

- 同步 I/O 是指，当用户线程发起 I/O 请求时，数据已经准备好了，需要把内核空间的数据拷贝到用户空间，此时用户线程会**等待拷贝完成**
- 异步 I/O 是指，当用户线程发起 I/O 请求时，数据已经准备好了，需要把内核空间的数据拷贝到用户空间，此时用户线程**不会等待拷贝完成**

同步 I/O 就是注册可读事件，由线程调用函数读取。异步就是通过 aio_read 函数，告诉内核，缓冲区的地址，内核数据拷贝完成后通知线程



### webserver两种高效的事件处理模式？

服务器通常需要处理三类事件：I/O 事件，信号以及定时事件，两种高效的事件处理模式分别是 Reactor 和 Proactor。

**Reactor模式**，要求主线程只负责监听文件描述符上是否有事件发生，如果有的话，就把该事件通知给工作线程，将 socket 可读可写事件放入到请求队列中，交给工作线程处理。除此之外，主线程不做其他实质性的工作。读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。

使用同步 I/O 实现 Reactor 模式的工作流程为：

1. 主线程向 epoll_wait 内核事件表内注册 socket 上的读就绪事件
2. 然后调用 epoll_wait 等待 socket 上有数据可读
3. 当 socket 上有数据可读的时候， epoll_wait 通知主线程，主线程把 socket 可读事件放入到请求队列
4. 然后唤醒一个工作线程，从 socket 上读取数据，处理客户请求，再往 epoll 内核事件表中注册该 socket 上的写就绪事件
5. 主线程调用 epoll_wait 等待 socket 可写
6. 当 socket 可写的时候， epoll_wait 通知主线程，主线程把 socket 可写事件放入到请求队列
7. 唤醒请求队列上的工作线程，向 socket 上写入服务器处理客户请求的结果

![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230218204203.png)



**Proactor 模式**，把所有的 I/O 操作都交给主线程和内核去处理（进行读写操作），工作线程仅仅负责业务逻辑。

使用异步 I/O 实现 Proactor 模式的工作流程：

1. 主线程调用 aio_read 函数向内核注册 socket 上的读完成时间，并告诉内核用户读缓冲区的位置，以及读操作完成时通知应用程序的方式
2. 主线程继续处理其他逻辑
3. 当 socket 上的数据被读到用户缓冲区后，内核向应用程序发送一个信号，通知应用程序数据已经可用
4. 应用程序预先定义的信号处理函数选择一个工作线程处理客户请求，工作线程完成后，调用 aio_write 函数向内核注册 socket 上的写完成事件，并告诉内核用户写缓冲区的位置，以及写操作完成时通知应用程序的方式
5. 主线程继续处理其他逻辑
6. 当用户缓冲区的数据被写入到 socket 后，内核向应用程序发送信号，通知应用程序数据已经发送完啦
7. 应用程序预先定义的信号处理函数选择一个工作线程进行善后处理，比如决定是否关闭 socket 

![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230218205644.png)



**二者区别：**在于 Reactor 模式的工作线程不仅负责业务逻辑处理，还需要完成数据的读写操作。而 Proactor 模式的工作线程仅负责业务逻辑处理。

**模拟 Proactor 模式：**用主线程来完成数据的读写过程，这样对于工作线程来说，就直接获得了数据读写的结果，只需要进行逻辑处理就行

使用同步 I/O 模型模拟的 Proactor 模式的工作流程如下

1. 主线程向 epoll 内核事件表注册 socket 上的读事件
2. 主线程调用 epoll_wait 等待 socket 上有数据可读
3. 当 socket 上有数据可读时，epoll_wait 通知主线程，主线程读取数据，然后封装成一个请求对象，插入到请求队列中
4. 唤醒一个工作线程，获得请求对象处理客户请求，然后向 epoll 内核事件表中注册 socket 上的写就绪事件
5. 主线程调用 epoll_wait 等待 socket 可写
6. 当 socket 可写时，epoll_wait 通知主线程，主线程向 socket 上写入服务器处理客户请求的结果

![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230218210647.png)





### 信号

三种常见的信号：SIGHUP，SIGPIPE，SIGURG

SIGHUP:

- SIGHUP 信号在**用户终端连接(正常或非正常)结束**时发出, 通常是在终端的控制进程结束时, 通知同一session内的各个作业, 这时它们与控制终端不再关联. 系统对SIGHUP信号的**默认处理是终止收到该信号的进程**。

SIGPIPE:

- 当往一个写端关闭的管道或socket连接中连续写入数据时会引发SIGPIPE信号,引发SIGPIPE信号的写操作将设置errno为EPIPE。在TCP通信中，当通信的双方中的一方close一个连接时，若另一方接着发数据，根据TCP协议的规定，会收到一个RST响应报文，若再往这个服务器发送数据时，系统会发出一个SIGPIPE信号给进程，告诉进程这个连接已经断开了，不能再写入数据。

SIGURG:

- 内核通知应用程序带外数据到达的方式有两种：一种就是利用ＩＯ复用技术的系统调用（如select）在接受到带外数据时将返回，并向应用程序报告socket上的异常事件。 
- 另一种方法就是使用SIGURG信号。



### 说一下 ET，LT，one_shot

首先，ET 和 LT 可以理解为是事件层面的，例如当有 socket 的数据到达的时候，会产生 EPOLLIN 事件

如果是 ET（边缘触发），那么该事件仅仅通知一次，不管到达的数据是否读完，下一次都不会再产生 EPOLLIN 事件，因此需要一次读完

如果是 LT（电平触发），那么该事件会通知多次，只要数据没有读完，那么就会一直产生 EPOLLIN 事件通知 socket 读取数据

而 one_shot，是线程层面的，当已经读取完某个 socket 的 EPOLLIN 事件的数据之后，会插入到请求队列中。这时候会从线程池中选择一个线程进行数据的解析处理（例如解析 HTTP 报文，生成 HTTP 响应报文等）。如果这个时候该 socket 上再次有 EPOLLIN 事件，主线程读取完数据之后插入到请求队列，这时会选择另外一个线程处理该请求，那么就会出现两个线程同时处理一个 socket 的请求这种情况。我们的设想肯定是一个 socket 的数据在某一时刻只由一个 工作线程处理，于是使用 one_shot。如果该 socket 注册 one_shot 事件，它规定操作系统最多触发其上注册的一个可读、可写或者异常事件，当触发了 EPOLLIN 事件之后，那么即使有后续的数据过来，也不会产生 EPOLLIN 事件，只有前面的线程处理完数据之后，才会重新注册 one_shot 事件。从而保证每个 socket 的数据不会**同时**被两个线程处理。

**一个socket在不用时期可以由不同工作线程处理，但同一时刻只有一个线程为之服务，保证了连接的完整性，避免了很多可能的竟态条件。**



### 说一下 select、poll、epoll 的区别是什么

首先，它们是 linux 下的三种不同的 I/O 复用的方式

- select 使用线性表描述文件描述符的集合，文件描述符有上限，poll 使用链表来描述，而 epoll 底层通过红黑树来描述，并且维护一个 ready list，把事件表中已经就绪的事件添加到这里，在使用 epoll_wait 调用时，观察这个 list 中有没有数据
- 对于 select 和 poll 来说，所有的文件描述符都是在用户态被加入其文件描述符集合，每次调用都需要把整个集合拷贝到内核态；而 epoll 是把整个文件描述符集合维护在内核态，每次添加文件描述符都需要执行系统调用
- select 和 poll 的最大开销来自内核判断是否有文件描述符就绪这个过程：每次执行 select 或 poll 调用，就会遍历整个文件描述符集合来判断是否有文件描述符就绪。而 epoll 不用，当有活动产生时，会自动触发 epoll 回调函数通知 epoll 文件描述符，然后内核把这些就绪的文件描述符放到 ready list 中等待 epoll_wait 调用
- select 和 poll 只能在 LT 模式下工作，而 epoll 同时支持 LT 和 ET 模式
- 当检测的文件描述符较少时，且每个文件描述符都比较活跃的情况下， select 和 poll 的效率较高。当监听的文件描述符较多且不太活跃的情况下，使用 epoll 较好



### 数据库登录说一下？

数据库登录分为

- 载入数据表，把数据库中的用户名和密码提取出来，用map容器存储
- 提取用户名和密码，通过解析请求报文的消息体，得到用户输入的用户名和密码
- 注册和登录校验，根据解析出来的用户名和密码，通过在 map 容器中查找，如果找得到说明是存在的，登录成功。注册的时候也会进行匹配，如果是一样的，说明存在相同的用户名，会注册失败
- 页面跳转，当登录成功时，会根据结果进行页面的跳转



### 你这个保存状态了吗？如果要保存，你会怎么做？（cookie和session）

Cookie 是一小段的文本信息。当客户端请求服务器的时候，如果服务器需要记录该用户的状态以及登录情况，就会在 response 的时候向客户端浏览器颁发一个 Cookie。客户端浏览器会把 Cookie 保存起来，当浏览器再次请求该服务器的时候，客户端会带上这一段 Cookie。然后服务器检查该 Cookie ，用来识别用户状态



Session是另外一种机制，不同的是 Cookie 保存在客户端浏览器上，而 Session 保存在服务器上，客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上，这就是 Session，客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。



### 登录中的用户名和密码你是load到本地，然后使用map匹配的，如果有10亿数据，即使l0ad到本地后hash，也是很耗时的，你要怎么优化？

- 数据结构的优化：为了保证数据库的一致性和完整性，在逻辑设计的时候会设计较多的表间关联，尽可能的降低数据的冗余
- 数据查询的优化：保证在实现功能的基础上，尽量减少对数据库的访问次数；通过搜索参数，尽量减少对表的访问行数，最小化结果集，从而减轻网络负担。
- 算法的优化：尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效
- 建立高效的索引：创建索引一般有以下两个目的：维护被索引列的唯一性和提供快速访问表中数据的策略。大型数据库有两种索引即簇索引和非簇索引，一个没有簇索引的表是按堆结构存储数据，所有的数据均添加在表的尾部，而建立了簇索引的表，其数据在物理上会按照簇索引键的顺序存储，一个表只允许有一个簇索引

对于大数据遍历的方法就是进行 hash，利用 hash 建立多级索引的方式加快用户验证

首先，把10亿用户信息，利用大致缩小 1000 倍的 hash 算法进行 hash，这时就获得了 100 万的 hash 数据，每一个 hash 数据代表着一个用户信息快（一级）

然后，再分别对这 100 万的 hash 数据再进行 hash，例如最后剩下 1000 个 hash 数据（二级）

在这种方式下，服务器只需要保存 1000 个二级 hash 数据，当用户请求登录的时候，先对用户信息进行一次 hash，找到对应信息块（二级？），再读取其对应的一级信息块，最终找到对应的用户数据



### 用的MySQL呀，redis了解吗？用过吗？

- 类型：MySQL是关系型数据库，主要用于存储持久化数据，把数据存储在硬盘中，读取速度慢。Redis 是 NOSQL，非关系型数据库，也就是缓存数据库，把数据存储到缓存中，读取速度快，但是保存的时间有限
- 运行机制：MySQL每次访问数据库的时候，都存在着 I/O 操作，如果反复频繁的访问数据库，会导致运行效率过慢，数据库的负载过高。Redis使用缓存，当浏览器执行请求时，首先会在缓存中进行查找，如果存在就获取，否则就访问数据库，读取速度快





### 为什么要用定时器？

处理定时任务，或者非活跃连接，节省系统资源



### 说一下定时器的工作原理

服务器的主循环为每个到来的连接创建一个定时器，并对每个连接进行定时，然后利用升序时间链表容器把所有的定时器串联起来。

然后利用 alarm 函数周期行地触发 SIGALRM 信号，信号处理函数就是利用管道，把该信号传递给主循环。主循环接收到信号事件，然后判断，如果是 SIGALRM 信号，就调用链表容器进行一次 tick()，该函数就是从头开始向后检查，如果有定时器的连接超时了，就会调用毁掉函数，释放连接资源。

当连接有数据处理的时候，会重置定时器的过期时间



### 双向链表啊，删除和添加的时间复杂度说一下？还可以优化吗？

刚好在头结点：添加是 O(1)，删除是 O(1)

刚好在尾节点：添加是 O(n)，删除是 O(1)

平均： 添加是 O(n)， 删除是 O(1)

因为从容器里面删除定时器的时候，是指定的 timer 



优化：

1. 在双向链表的基础上优化，添加在尾节点的事件复杂度可以优化，在添加新的定时器的时候，除了检测新定时器是否在小于头结点定时器的时间外，再先检测新定时器是否在大于尾节点定时器的事件，都不符合再使用常规插入？？？？？？？？？？？？？？？？？
2. 不使用双向链表，使用最小堆结构可以进行优化



### 最小堆优化？说一下时间复杂度和工作原理

时间复杂度：添加 O(lgn)，删除 O(1)

工作原理：

把所有定时器中超时事件最小的定时器的超时值作为 alarm 函数的定时值，这样，一旦定时任务处理函数 tick（） 被调用，超时时间最小的定时器必然到期，就可以在 tick函数中处理该定时器，然后，再次从剩余的定时器中找出超时事件最小的一个堆，并把这段最小时间设置为下一次 alarm 函数的定时值，这样就实现了较为精确的定时





### 说下你的日志系统的运行机制？

单例模式（局部静态变量懒汉模式）获取实例

首先主线程一开始对日志类进行初始化，Log::get_instance() -> init() ，初始化实例。初始化之后，服务器按照当前启动的时间创建日志文件（前缀为时间，后缀是自定义的 log 文件名，并记录创建日志的时间 day 和行数 count ）。如果是异步（通过是否设置队列大小判断是否异步，0为同步），工作线程将要写的内容放进阻塞队列，还创建了写线程用于在阻塞队列里取出一个内容（指针），写入日志

其他功能模块调用 write_log() 函数写日志，实现了日志的分级，份文件，按天按行分类的格式化输出内容



### 为什么要异步？和同步的区别是什么？

因为同步日志，写入函数与工作线程串行执行，由于涉及到 I/O 操作，在单条日志比较大的时候，同步模式会组设整个处理流程，服务器所能处理的并发能力将会下降，尤其是在峰值的时候，写日志可能会成为系统的瓶颈

而异步日志采用了 生产者-消费者 模型，工作线程把所写的日志内容先存入缓冲区，写线程从缓冲区中取出内容，写入日志，并烦恼歌力较高



### 什么是生产者消费者模式？

某个模块负责产生数据，这些数据由另外一个模块来负责处理（此处的模块是广义的，可以是进程，线程，类，函数等）。

产生数据的模块，就形象的称为 生产者，处理数据的模块，称为消费者

另外还需要一个缓冲区处于生产者和消费者之间，作为一个中介。生产者把数据放入缓冲区，而消费者从缓冲区取出数据。

![](https://cdn.jsdelivr.net/gh/vaesong/Images//20230318153820.png)



### 缓冲区用什么实现？

缓冲区使用循环数组实现队列，作为二者共享的缓冲区



### 现在你要监控一台服务器的状态，输出监控日志，请问如何将该日志分发到不同的机器上

同一个机器：使用观察者模式（有的叫发布订阅模式）

多个机器：借助 redis 数据库的消息队列的发布订阅模式，实现分布式日志系统

为了便于故障排查，或服务器状态分析，看是否需要维护；可以使用消息队列进行消息的分发，例如mqtt、rabitmq等等



### **Webbench是什么，介绍一下原理**？

Webbench是一款轻量级的网址压力测试工具，可以实现高达3万的并发测试

原理：父进程 fork 若干个子进程，每个子进程在用户要求的时间或者默认的时间内对目标 web 循环发出实际访问请求，父子进程通过管道进行通信，子进程通过管道写端向父进程传递在若干次请求访问完毕后记录到的总信息。父进程通过管道读取子进程附送的相关信息，父进程在时间到后结束，父进程在所有子进程退出后同济并给用户显示最后的测试结果，然后退出



### RAII 是什么？

当我们在获取资源的时候，经常在使用之后会忘记销毁申请的资源，于是引入了 RAII 机制

该方法就是设计一个封装资源的类，在类的构造函数里面，进行初始化，获取资源。在类的析构函数里进行资源的释放

这样操作之后，在局部作用域对类进行实例化，这样就会自动获得资源，当离开作用域之后，该实例被自动销毁，调用析构函数，从而自动释放资源









# 设计模式

