# 编译 ffmpeg

```shell
./configure --prefix=/usr/local/ffmpeg --disable-static --enable-shared --enable-gpl --enable-libx264 --enable-postproc

sudo make -j 4

sudo make install
```





# 常用命令解释

## ffmpeg

### 直接采集和播放音频

```ffmpeg
ffmpeg -f alsa -i hw:0 out.wav

-f: 指定用于捕获和录制音频和视频的设备,linux系统中使用 ALSA（用于音频捕获）或 v4l2（用于视频捕获）
-i: 指定了输入设备，默认音频是 default ， [视频设备]:[音频设备]
```

```ffmpeg
ffplay out.wav
```



### 直接生成AAC文件

```ffmpeg
ffmpeg -i XXX.mp4 -vn -c:a libfdk_aac -ar 44100 -channels 2 -profile:a aac_he_v2 3.aac

-i: 指定的视频来源
-vn: 不采样视频
-c:a 代表编码器，其中 a 代表audio音频编码器，指定为 libfdk_aac
-ar: 代表音频的采样率
-channels: 代表采样通道
-profile:a 对使用的音频编码器进行参数设置
```





## ffplay

编码之后的音视频是可以直接播放的，但是对于没有处理的原始数据，需要指定一些参数才能播放



### 音频

pcm格式是原始的、未经压缩的音频格式

```C
ffplay -f s16le -ac 2 -ar 44100 test.pcm

-f: 指定pcm数据的格式，s16se(signed 16 bits little endian, 有符号 16 位小端) 
-ac: 音频通道
-ar: 音频采样率
```

aac是编码之后的格式

```C

```



### 视频

从视频中解码出 YUV 格式

```C
ffmpeg -i bilibili.mp4 -an -c:v rawvideo -pix_fmt yuv420p out.yuv

-i: 视频
-an: 不用音频
-c:v: 视频解码，成 rawvideo
-pix_fmt: 输出的图片格式
```



播放 YUV 视频

```C
ffplay -pix_fmt yuv420p -s 1920x1080 out.yuv
ffplay -f rawvideo -pix_fmt yuyv422 -s 1280x720 -framerate 10 test.yuv
    
-f: 指定视频格式，原始视频
-pix_fmt: 指定像素的格式
-s: 播放的分辨率
-framerate: 帧率
-vf extractplanes='y'  滤波器，只提取出 Y 分量
```



## 命令分类

### 基本信息查询命令

![](https://raw.githubusercontent.com/vaesong/Images/master/20231113203541.png)



### 录制命令

- 录制视频

```Shell
ffmpeg -f v4l2 -i /dev/video0 -c:v libx264 -pix_fmt yuv420p output.mp4

-f: 指定用于捕获和录制音频和视频的设备,linux系统中使用 ALSA（用于音频捕获）或 v4l2（用于视频捕获）
-i: /dev/video0，表示摄像头的文件
-c:v: 指定编码格式
```

- 录制桌面

```Shell
# 直接输出 yuv 文件
ffmpeg -f x11grab -r 30 -i :0.0 desk.yuv
# 输出编码后的文件
ffmpeg -f x11grab -r 30 -s 1920x1080 -i :0.0 -vcodec libx264 -preset ultrafast -crf 18 output.mp4

-f: 使用 x11grab 框架来捕获屏幕，这是 X Window 系统的标准屏幕捕获方法
-r: 帧率
-i: 0: 是音频， ：0 是视频， :0.0 是桌面
-vcodec： 编码器
-preset： 编码器的预设，ultrafast 提供了更快的编码速度
-crf: 设置常量率因子，影响视频质量和文件大小。数字越小，质量越高，文件大小也越大。18 到 24 是一个合理的范围
最后是输出的文件

# 录制桌面出来的格式是 bgr0
ffplay -s 5120x1600 -pix_fmt bgr0 desk.yuv
```

- 录制声音

```Shell
ffmpeg -f alsa -i hw:0 out.wav

-f: 指定用于捕获和录制音频和视频的设备,linux系统中使用 ALSA（用于音频捕获）或 v4l2（用于视频捕获）
-i: 指定了输入设备，默认音频是 default 
```



### 分解/复用命令

- mp4 直接转 flv

```Shell
ffmpeg -i input.mp4 -c:v libx264 -c:a aac -strict experimental output.flv

```





### 处理原始数据命令

- 提取 YUV 数据

```Shell
ffmpeg -i input.mp4 -an -c:v rawvideo -pix_fmt yuv420p out.yuv

-i: 输入
-an: 不用音频
-c:v: rawvideo 原始数据格式
-pix_fmt: 图像存储格式
```

- 提取 pcm 数据

```Shell
ffmpeg -i input.mp4 -vn -ar 44100 -ac 2 -f s16le out.pcm

-i: 指定的视频来源
-vn: 不采样视频
-ar: 代表音频的采样率
-ac: 音频通道数
-f: 数据格式， 16 bit
```





### 裁剪与合并命令

- 裁剪

```shell
ffmpeg -i input.mp4 -ss 00:00:00 -t 10 out.ts

-ss: 裁剪开始时间
-t: 持续时间
```

- 合并

```Shell
ffmpeg -f concat -i input.txt out.mp4

-f: concat 表示合并。使用 concat 滤镜进行视频合并时，所有视频文件应该具有相同的格式和编码设置
-i: 视频文件名列表

# input.txt 格式
file '1.ts'
file '2.ts'
```





### 图片/视频互转命令

- 视频转图片

```shell
ffmpeg -i input.flv -r 1 -f image2 image-%3d.jpeg

-r: 帧率是 1，每秒生成 1 张
-f: 输出的图片格式
```

- 图片转视频

```Shell
ffmpeg -i image-%3d out.mp4

```





### 直播相关命令

- 直播推流

```shell
ffmpeg -re -i out.mp4 -c copy -f flv rtmp://server/live/streamName

-re: 让帧率与真正的帧率保持同步
-i: 表示 input 输入
-c: 表示编解码， -c:a 是音频， -c:v 是视频， copy 就是不变
-f: 表示推流出去的格式，一般是 flv
最后就是地址
```

- 直播拉流

```Shell
ffmpeg -i rtmp://server/live/streamName -c copy dump.flv

-i: 输入
-c: 编码
最后是输出文件
```





### 各种滤镜命令

- 裁剪

```Shell
ffmpeg -i input.mp4 -vf crop=in_w-200:in_h-200 -c:v libx264 -c:a copy out.mp4

-i: 输入数据
-vf: 视频滤镜，crop 裁剪，宽度和高度减少200
-c:v 视频编码器
-c:a 音频编码器
```





# 声音

## 量化标准

- 采样大小，主要是看使用多少 bit 来存储一个采样的数据
- 采样率，一秒采样多少次
- 声道数，几个声道采样

![](https://raw.githubusercontent.com/vaesong/Images/master/20231107102301.png)



## 编码过程

![](https://raw.githubusercontent.com/vaesong/Images/master/20231107113318.png)



## 重采样

- 从设备采集的音频数据和编码器要求的数据不一致
- 扬声器要求的音频数据和要播放的音频数据不一致
- 更加方便运算

步骤：

- 创建重采样上下文
- 设置参数
- 初始化重采样
- 进行重采样

常见 API：

```C
//分配重采样上下文描述符
swr_alloc_set_opts
struct SwrContext *swr_alloc_set_opts(struct SwrContext *s,
                                      int64_t out_ch_layout, enum AVSampleFormat out_sample_fmt, int out_sample_rate,
                                      int64_t  in_ch_layout, enum AVSampleFormat  in_sample_fmt, int  in_sample_rate,
                                      int log_offset, void *log_ctx);

//初始化
swr_init
int swr_init(struct SwrContext *s);

//创建缓冲区
av_samples_alloc_array_and_samples
int av_samples_alloc_array_and_samples(uint8_t ***audio_data, int *linesize, int nb_channels,
                                       int nb_samples, enum AVSampleFormat sample_fmt, int align);

//进行转换，从输入缓冲区放到输出缓冲区
swr_convert
int swr_convert(struct SwrContext *s, uint8_t **out, int out_count,
                                const uint8_t **in , int in_count);

swr_free
void swr_free(struct SwrContext **s);
```

```C
#include "test.h"

void haha(){
	//上下文，类似于文件描述符
    AVFormatContext * fmt_ctx = NULL;
    AVDictionary * options = NULL;
	//设备名称
    char * devicename = "hw:0";
    int ret = 0;
    char errors[1024] = {0};

    //注册设备
    avdevice_register_all();

    //设置格式，指定用于捕获的设备类型
    const AVInputFormat* iformat = av_find_input_format("alsa");

    //打开设备
    if((ret = avformat_open_input(&fmt_ctx, devicename, iformat, &options)) < 0){
        av_strerror(ret, errors, 1024);
        fprintf(stderr, "Failed to open audio device, [%d] %s\n", ret, errors);
        return;
    }


    //创建文件
    char *out = "/home/vaesong/Data/Workspace/Audio_Video/1-Introduction/audio/test.pcm";
    FILE *outfile = fopen(out, "wb+");

    
    //重采样设置
    //重采样上下文
    SwrContext * swr_ctx = NULL;
    //重采样的分配上下文
    swr_ctx = swr_alloc_set_opts(NULL,              //ctx
                        AV_CH_LAYOUT_STEREO,        //输出的channel布局
                        AV_SAMPLE_FMT_S16,          //输出的采样格式
                        44100,                      //输出的采样率
                        AV_CH_LAYOUT_STEREO,        //输入的channel布局
                        AV_SAMPLE_FMT_FLT,          //输入的采样格式
                        44100,                      //输入的采样率
                        0, NULL);               

    //如果是 0 说明失败了
    if(!swr_ctx){
        printf("Allocate Failed");
    }

    //重采样初始化
    if(swr_init(swr_ctx) < 0){
        printf("Init Failed");
    }

    //转换之前先定义输入和输出的数据存放的地址
    uint8_t **src_data = NULL;
    int src_linesize = 0;

    uint8_t **dst_data = NULL;
    int dst_linesize = 0;
    
    //这里的采样个数代表的是单通道个数 16384(采样的数据大小)/4(每个数据的采样大小) = 4096 / 2(通道数) = 2048
    av_samples_alloc_array_and_samples(&src_data,           //输入缓冲区的地址
                                        &src_linesize,      //缓冲区的大小
                                        2,                  //通道个数
                                        2048,               //单通道采样个数
                                        AV_SAMPLE_FMT_FLT,  //采样格式
                                        0);

    av_samples_alloc_array_and_samples(&dst_data,           //输入缓冲区的地址
                                        &dst_linesize,      //缓冲区的大小
                                        2,                  //通道个数
                                        2048,               //单通道采样个数 16384不变，每个数据的大小变成了 16bit 2B 才对，不应该是 16384 / 2 = 8192 / 2 = 4096 ？
                                        // 16384并不是不变的，而是根据采样大小确定，每个采样大小变成了 16bit 2B，每个通道采样 2048 个，总大小是 2028 * 2 * 2 = 8192
                                        AV_SAMPLE_FMT_S16,  //采样格式
                                        0);

    printf("src_linesize: %d\n", src_linesize);
    printf("dst_linesize: %d\n", dst_linesize);
    printf("============\n");

    //初始化数据包
    AVPacket pkt;
    av_init_packet(&pkt);
    int count = 0;
	//读取数据
    while( (ret = av_read_frame(fmt_ctx, &pkt)) == 0 && count++ < 50){
        av_log(NULL, AV_LOG_INFO, "packet size is %d(%p)\n", pkt.size, pkt.data);

        //进行内存拷贝，按字节拷贝的
        memcpy((void*)src_data[0], (void*)pkt.data, pkt.size);

        //在这里进行转换，
        swr_convert(swr_ctx,                        //重采样的上下文描述符
                    dst_data,                       //目的缓冲区
                    2048,                           //单通道的采样个数
                    (const uint8_t **)src_data,     //源缓冲区
                    2048);                          //单通道的采样个数

        //写入文件
        fwrite(dst_data[0], 1, dst_linesize, outfile);
        fflush(outfile);

        // fwrite(pkt.data, 1, pkt.size, outfile);

        //释放 pkt 数据描述符？
        av_packet_unref(&pkt);
    }

    //关闭文件
    fclose(outfile);
    
    //关闭设备
    avformat_close_input(&fmt_ctx);

    //关闭重采样上下文描述符
    if(src_data){
        av_freep(&src_data[0]);
    }
    if(dst_data){
        av_freep(&dst_data[0]);
    }
    swr_free(&swr_ctx);


    av_log_set_level(AV_LOG_DEBUG);
    av_log(NULL, AV_LOG_DEBUG, "Finished\n");
    return;
}

int main(){

    haha();

    return 0;
}
```



## 采集读取音频

- 注册设备
- 设置采集方式 avfoundation/dshow/alsa
- 打开音频设备

```C
void haha(){
	//上下文，类似于文件描述符
    AVFormatContext * fmt_ctx = NULL;
    AVDictionary * options = NULL;
	//设备名称
    char * devicename = "hw:0";
    int ret = 0;
    char errors[1024] = {0};

    //注册设备
    avdevice_register_all();

    //设置格式，指定用于捕获的设备类型
    AVInputFormat* iformat = av_find_input_format("alsa");

    //打开设备
    if((ret = avformat_open_input(&fmt_ctx, devicename, iformat, &options)) < 0){
        av_strerror(ret, errors, 1024);
        printf(stderr, "Failed to open audio device, [%d] %s\n", ret, errors);
        return;
    }

    //初始化数据包
    AVPacket pkt;
    av_init_packet(&pkt);
    int count = 0;
	//读取数据
    while( (ret = av_read_frame(fmt_ctx, &pkt) && count++ < 100) == 0){
        printf("pkt size is %d \n", pkt.size);
    }

    av_log_set_level(AV_LOG_DEBUG);
    av_log(NULL, AV_LOG_DEBUG, "hello, world\n");
    return;
}
```





## 编码

- 创建编码器
- 创建上下文
- 打开编码器
- 送数据给编码器
- 编码
- 释放资源

```C
//创建编码器
avcodec_find_encoder
    
//创建上下文
avcodec_alloc_context3
    
//打开编码器
avcodec_open2
    
AVCodecContext * open_coder(){
    //找到编码器并打开
    AVCodec * codec = avcodec_find_encoder_by_name("libfdk_aac");

    //创建 codec 上下文
    AVCodecContext * codec_ctx = avcodec_alloc_context3(codec);

    //设置参数
    codec_ctx->sample_fmt = AV_SAMPLE_FMT_S16;          //输入音频的采样大小
    codec_ctx->sample_rate = 44100;                     //输入音频的采样率
    codec_ctx->channel_layout = AV_CH_LAYOUT_STEREO;    //输入音频的 通道布局 channel layout，双声道，左右
    codec_ctx->channels = 2;                            //输入音频的通道数
    codec_ctx->bit_rate = 0;                            //设置为 0 ，会根据 profile 的设置进行选择
    codec_ctx->profile = FF_PROFILE_AAC_HE_V2;          // AAC_LC: 128K, AAC_HE: 64K, AAC HE V2: 32K

    //打开编码器
    if(avcodec_open2(codec_ctx, codec, NULL) < 0){
        printf("Open Codec Failed!");
        return NULL;
    }

    return codec_ctx;
}
```



AVFrame 保存的是未编码的数据

```C
static AVFrame * create_frame(){
    AVFrame * frame = av_frame_alloc();

    if(!frame){
        printf("Frame Allocate Failed");
    }

    frame->nb_samples     = 2048;                       //单通道采样个数,最大就是 2048
    frame->format         = AV_SAMPLE_FMT_S16;          //采样大小
    frame->channel_layout = AV_CH_LAYOUT_STEREO;        //channel布局
    frame->channels       = 2;                          //channel数量

    //分配内存，一开始是 -1
    av_frame_get_buffer(frame, 0);
    printf("frame.size:%d\n", frame->pkt_size);

    if(!frame->data[0]){
        printf("Frame Buffer Allocate Failed");
        av_frame_free(&frame);
        return NULL;
    }

    return frame;
}
```



AVPacket 保存的是编码后的数据

```C
AVPacket * newpkt = av_packet_alloc();
if(!newpkt){
    printf("Packet Allocate Failed");
}
```

需要提前给他们分配空间，然后调用编码函数

```C
//这里是采集的单通道个数太多了，分成两批传到编码器里进行编码
        int remaining_samples = 4096;
        int nb_samples = 2048;
        uint8_t *data_ptr = pkt.data;

        while (remaining_samples >= nb_samples) {
            //拷贝数据到 frame 的位置，进行编码
            memcpy((void*)(frame->data[0]), (void*)data_ptr, nb_samples * 4);

            //把数据发送给编码器，对其进行编码
            ret = avcodec_send_frame(c_ctx, frame);
            // printf("===========\n");
            // printf("remaining_samples: %d\n", remaining_samples);
            data_ptr += (nb_samples * 4);
            remaining_samples -= nb_samples;
            
        }

//编码
static void encode(AVCodecContext * ctx, AVFrame * frame, AVPacket* pkt, FILE * output){
    int ret = 0;
    //获取编码后的值
    while(ret >= 0){
        ret = avcodec_receive_packet(ctx, pkt);

        if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){
            return;
        }
        else if(ret < 0){
            printf("Error, encoding audio frame\n");
            exit(-1);
        }

        //写入文件
        fwrite(pkt->data, 1, pkt->size, output);
        fflush(output);
    }
}
```







# 视频

分辨率符合宽高比 640 * 480

常见的宽高比 16：9 / 4：3

没有编码的视频码流：像素大小 * 分辨率 * 帧数



## YUV

[YUV解释](https://www.cnblogs.com/mjios/p/14686970.html#toc_title_0)

常见的格式：

- 4：4：4
- 4：2：2
- 4：2：0

这里的图片实际是不合理的，其实是采样比

对于 4：4：4 来说，每采集一个像素的 Y ，就会采集一个 U和 V，也就是每个像素都有自己的 Y、U、V

对于 4：2：2 来说，每采集两个像素的 Y，才会采集一个 U 和 V，也就是两个像素，共用一个 U、V，其中 Y 是每个像素独有的

对于 4：2：0 俩说，每采集四个像素的 Y，才会采集一个 U 和 V，也就是四个像素，共用一个 U、V

![](https://raw.githubusercontent.com/vaesong/Images/master/20231109104058.png)

![image-20231109104201751](/home/vaesong/.config/Typora/typora-user-images/image-20231109104201751.png)

![image-20231109104213675](/home/vaesong/.config/Typora/typora-user-images/image-20231109104213675.png)



### 存储格式

- Planar（平面）
  - Y、U、V分量分开单独存储
  - 名称通常以字母p结尾
- Semi-Planar（半平面）
  - Y分量单独存储，U、V分量交错存储
  - 名称通常以字母sp结尾
- Packed（紧凑）
  - 或者叫**Interleaved** （交错）
  - Y、U、V分量交错存储

#### Planar

#### Packed

![](https://raw.githubusercontent.com/vaesong/Images/master/20231109112300.png)

## 采集读取视频

- 注册设备
- 设置采集方式 `v4l2`
- 打开视频设备

```C
void test_video(){

    //输入设备上下文
    AVFormatContext * fmt_ctx = NULL;
    AVDictionary * options = NULL;
    char errors[1024] = {};
    
    //设备 [video]:[audio]
    //对于视频来说，0： 摄像头， 1：桌面
    char * device = "/dev/video0";
    int ret = 0;

    //注册设备
    avdevice_register_all();

    //设置输入格式，指定设备
    const AVInputFormat * iformat = av_find_input_format("v4l2");

    //对于视频来说，需要设置分辨率等
    av_dict_set(&options, "video_size", "1280x720", 0);
    av_dict_set(&options, "framerate", "10", 0);
    //av_dict_set(&options, "pixel_format", "nv12", 0);


    //打开设备
    if((ret = avformat_open_input(&fmt_ctx, device, iformat, &options)) < 0){
        av_strerror(ret, errors, 1024);
        fprintf(stderr, "Failed to open device, [%d] %s\n", ret, errors);
        return;
    }

    //创建文件
    char *out = "/home/vaesong/Data/Workspace/Audio_Video/1-Introduction/video/test.yuv";
    FILE *outfile = fopen(out, "wb+");

    //初始化数据包
    AVPacket pkt;
    av_init_packet(&pkt);
    int count = 0;
	//读取数据
    while( (ret = av_read_frame(fmt_ctx, &pkt)) == 0 && count++ < 100){
        av_log(NULL, AV_LOG_INFO, "packet size is %d(%p)\n", pkt.size, pkt.data);
      
        //写入文件
        fwrite(pkt.data, 1, pkt.size, outfile);
        fflush(outfile);

        //释放 pkt 数据描述符？
        av_packet_unref(&pkt);
    }

    if(outfile){
        fclose(outfile);
    }

}
```



## 编码

[声网参考文档，可查看码流](https://docportal.shengwang.cn/cn)

[知乎 H264 文章](https://zhuanlan.zhihu.com/p/605149388?utm_id=0&wd=&eqid=ab07d33200075d8e000000056487d6a3)

### GOP

分组，每一组里面的帧差别小

算法是：在相邻几幅图像画面中，一般有差别的像素只有10%以内的点,亮度差值变化不超过2%，而色度差值的变化只有1%以内，我们认为这样的图可以分到一组

1. GOP即Group of  picture（图像组），指两个I帧之间的距离(下图所说的视频序列就是GOP)，Reference（参考周期）指两个P帧之间的距离，可以理解为跟序列差不多意思，就是一段时间内变化不大的图像集，比较说GOP为120,如果是720 p60  的话,那就是2s一次I帧。一个I帧所占用的字节数大于一个P帧，一个P帧所占用的字节数大于一个B帧。所以在码率不变的前提下，GOP值越大，P、B帧的数量会越多，平均每个I、P、B帧所占用的字节数就越多，也就更容易获取较好的图像质量；Reference越大，B帧的数量越多，同理也更容易获得较好的图像质量。
2. GOP结构一般有两个数字，如M=3，N=12。M指定I帧和P帧之间的距离，N指定两个I帧之间的距离。上面的M=3，N=12，GOP结构为：IBBPBBPBBPBBI。在一个GOP内I frame解码不依赖任何的其它帧，p frame解码则依赖前面的I frame或P frame，B frame解码依赖前最近的一个I  frame或P frame 及其后最近的一个P frame。



- **I 帧**：关键帧，采用**帧内压缩**技术，IDR帧属于 I 帧。I帧是帧组GOP的基础帧(第一帧),在一组中只有一个I帧。在一些编码方案中，**也可能在GOP中的其他位置插入额外的I帧**。这些非首位的I帧不会引起解码器状态的重置

- **P 帧**： 向前参考帧，压缩时，只参考前面已经处理的帧**（I 帧、P 帧）**，采用**帧间压缩**技术，占 I 帧的一半大小
- **B 帧**：双向参考帧，压缩时，既参考前面已经处理的帧，也参考后面的帧，帧间压缩技术，占 I 帧的1/4 大小，**不会参考 B 帧**。B帧压缩率高，但是解码时CPU会比较累
- **IDR 帧**：不依赖任何帧，解码器立即刷新，**每个GOP中的第一帧就是 IDR 帧**。将GOP中首个I帧要和其他I帧区别开，把第一个I帧叫IDR，这样方便控制编码和解码流程，所以IDR帧一定是I帧，但I帧不一定是IDR帧；IDR帧的作用是立刻刷新,使错误不致传播,从IDR帧开始算新的序列开始编码。I帧有被跨帧参考的可能,IDR不会

> DTS：**解码时间戳**，这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据
>
> PTS：**显示时间戳**，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据

![](https://raw.githubusercontent.com/vaesong/Images/master/20231115214449.png)

**时间基与时间戳的概念**

在FFmpeg中，时间基(time_base)是时间戳(timestamp)的单位，时间戳值乘以时间基，可以得到实际的时刻值(以秒等为单位)。例如，如果一个视频帧的dts是40，pts是160，其time_base是1/1000秒，那么可以计算出此视频帧的解码时刻是40毫秒(40/1000)，显示时刻是160毫秒(160/1000)。FFmpeg中时间戳(pts/dts)的类型是int64_t类型，把一个time_base看作一个时钟脉冲，则可把dts/pts看作时钟脉冲的计数。



- SPS 和 PPS： 一般是同时出现，在 IDR 帧之前。
  - SPS（序列参数集），作用于一串连续的视频图像，如 seq_parameter_set_id、帧数及 POC（picture order count）的约束、参考帧数目、解码图像尺寸和帧场编码模式选择标识等
  - PPS（图像参数集），作用于视频序列中的图像。如 pic_parameter_set_id、熵编码模式选择标识、片组数目、初始量化参数和去方块滤波系数调整标识等

因此在解码的时候，先解码 I 帧，由于 B 帧依赖前后帧，于是先解码 P 帧，最后解码 B 帧

![](https://raw.githubusercontent.com/vaesong/Images/master/20231109200435.png)



### H264

花屏：GOP分组中有帧丢失，会造成解码端的图像发生错误，就会出现马赛克。（丢失 I 帧，主要是 P 帧，B 帧）

卡顿：有帧丢失，直接丢掉一组帧，直到下一个 IDR 帧重新刷新图像

**帧内压缩技术**：

![帧内预测模式](https://raw.githubusercontent.com/vaesong/Images/master/20231109211151.png)



![](https://raw.githubusercontent.com/vaesong/Images/master/20231109213525.png)

#### 码流

H264码流是由一个个的NAL单元组成，其中SPS、PPS、IDR和SLICE是NAL单元某一类型的数据。

- **VCL（Video Coding Layer）**，VCL层是对核心算法引擎、块、宏块及片的语法级别的定义，负责有效表示视频数据的内容，最终输出编码完的数据SODB
- **NAL（Network Abstraction Layer）**，NAL层定义了片级以上的语法级别（如序列参数集和图像参数集，针对网络传输），负责以网络所要求的恰当方式去格式化数据并提供头信息，以保证数据适合各种信道和存储介质上的传输。



SODB（String Of Data Bits）： 数据比特串，是经过 VCL 编码之后的原始数据

RBSP（Raw Byte Sequence Payload）：在 SODB 的基础上进行补齐

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111151235.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231110211524.png" style="zoom:50%;" />



#### NAL

![](https://raw.githubusercontent.com/vaesong/Images/master/20231111150600.png)

1. 1帧（一幅图像） = 1~N个片（slice） //也可以说1到多个片为一个片组
2. 1个片 = 1~N个宏块（Marcroblock）
3. 1个宏块 = 16*16的YUV数据（原始视频采集数据）



##### SPS

- H264 Profile：对视频压缩特性的描述，Profile越高，就说明采用了越高级的压缩特性
- H264 Level：对视频的描述，Level 越高，视频的码率，分辨率，fps越高

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111160932.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111161040.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111161142.png" style="zoom: 67%;" />



相关的一些参数

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111161432.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111161936.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111161957.png" style="zoom:50%;" />

##### PPS

常见参数

![](https://raw.githubusercontent.com/vaesong/Images/master/20231111162208.png)



##### Slice Header

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231111162515.png" style="zoom:50%;" />



NAL单元的头部是由forbidden_bit(1bit)，nal_reference_bit(2bits)（优先级），nal_unit_type(5bits)（类型）三个部分组成的

1. F(forbiden):禁止位，占用NAL头的第一个位，当禁止位值为1时表示语法错误；
2. NRI:取00~11,似乎指示这个NALU的重要性,如00的NALU解码器可以丢弃它⽽不影响图像的回放,0～3，取值越⼤，表示当前NAL越重要，需要优先受到保护。如果当前NAL是属于参考帧的⽚，或是序列参数集，或是图像参数集这些重要的单位时，本句法元素必需⼤于0。
3. Type:NAL单元数据类型，也就是标识该NAL单元的数据类型是哪种，占用NAL头的第四到第八个位；

![](https://raw.githubusercontent.com/vaesong/Images/master/20231111151745.png)





![](https://raw.githubusercontent.com/vaesong/Images/master/20231111151937.png)

码流数据分层图

![](https://raw.githubusercontent.com/vaesong/Images/master/20231111152614.png)

## 编码实战

- 打开编码器
- 转换 NV12 到 YUV420P
- 准备编码数据 AVFrame
- H264编码

```C
#include <libavdevice/avdevice.h>
#include <libavcodec/avcodec.h>
#include <libavformat/avformat.h>
#include <libavutil/avutil.h>
#include <libavfilter/avfilter.h>
#include <libswscale/swscale.h>
#include <libavutil/imgutils.h>
#include <stdio.h>
#define WIDTH 1280
#define HEIGHT 720

/**
 * @brief 打开视频设备
 * 
 * @return AVFormatContext* 
 */
AVFormatContext* open_dev(){
    //输入设备上下文
    AVFormatContext *fmt_ctx = NULL;
    AVDictionary * options = NULL;
    char errors[1024] = {};
    
    //设备 [video]:[audio]
    //对于视频来说，0： 摄像头， 1：桌面
    char * device = "/dev/video0";
    int ret = 0;

    //注册设备
    avdevice_register_all();

    //设置输入格式，指定设备
    const AVInputFormat * iformat = av_find_input_format("v4l2");

    //对于视频来说，需要设置分辨率等
    av_dict_set(&options, "video_size", "1280x720", 0);
    av_dict_set(&options, "framerate", "10", 0);
    // av_dict_set(&options, "pixel_format", "nv12", 0);


    //打开设备
    if((ret = avformat_open_input(&fmt_ctx, device, iformat, &options)) < 0){
        av_strerror(ret, errors, 1024);
        fprintf(stderr, "Failed to open device, [%d] %s\n", ret, errors);
        exit(-1);
    }

    return fmt_ctx;
}

/**
 * @brief 打开视频编码器
 * 
 * @param enc_ctx 编码器的上下文
 * @param width 视频图像的宽度
 * @param height 视频图像的高度
 */
void open_encoder(AVCodecContext ** enc_ctx, int width, int height){
    //编码器，设备?格式？
    const AVCodec *codec = avcodec_find_encoder_by_name("libx264");
    
    if(!codec){
        printf("Codec libx264 not found\n");
        exit(-1);
    }

    *enc_ctx = avcodec_alloc_context3(codec);
    
    if(!enc_ctx){
        printf("Could not allocate video codec context\n");
        exit(-1);
    }

    //设置 SPS/PPS 相关数据
    (*enc_ctx)->profile = FF_PROFILE_H264_HIGH_444;
    (*enc_ctx)->level = 31;                             //表示 LEVEL 是 3.1

    //设置分辨率
    (*enc_ctx)->width = width;
    (*enc_ctx)->height = height;

    //GOP参数
    (*enc_ctx)->gop_size = 20;      //大概20/30帧为一组
    (*enc_ctx)->keyint_min = 20;    //最小的 I 帧之间的距离

    //参考帧的数量，解码器缓冲区中存放的数量
    (*enc_ctx)->refs = 5;

    //设置输入的 YUV 格式
    (*enc_ctx)->pix_fmt = AV_PIX_FMT_YUV420P;

    //设置码率
    (*enc_ctx)->bit_rate = 2000000;  // 2000 Kbps = 250 KB/S

    //设置帧率
    (*enc_ctx)->time_base = (AVRational){1, 10};    //帧间的间隔，一帧播放的时间？
    (*enc_ctx)->framerate = (AVRational){10, 1};

    int ret = avcodec_open2((*enc_ctx), codec, NULL);

    if(ret < 0){
        printf("Could not open codec: %s\n", av_err2str(ret));
        exit(-1);
    }
}

/**
 * @brief Create a frame object
 * 
 * @param width 
 * @param height 
 * @return AVFrame* 
 */
AVFrame* create_frame(int width, int height){
    AVFrame *frame = NULL;

    //首先分配一个 frame 结构体
    frame = av_frame_alloc();

    if(!frame){
        printf("Error, No Memory!\n");
        exit(-1);
    }

    //然后设置参数
    frame->width = width;
    frame->height = height;
    
    frame->format = AV_PIX_FMT_YUV420P;        //设置数据格式  

    //最后真的分配数据空间
    int ret = av_frame_get_buffer(frame, 32);   //按照 32 位对齐
    if(ret < 0){
        printf("Frame buffer allocate failed!\n");
        exit(-1);
    }

    return frame;
}

/**
 * @brief Create a packet object
 * 
 * @return AVPacket* 
 */
AVPacket* create_packet(){
    AVPacket* newpkt = NULL;
    newpkt = av_packet_alloc();

    if(!newpkt){
        printf("Packet allocate failed!\n");
        exit(-1);
    }

    return newpkt;
}

/**
 * @brief 创建scale上下文
 * 
 * @param sws_ctx 
 */
void init_sws(struct SwsContext **sws_ctx){
    *sws_ctx = sws_getContext(
            WIDTH,              // 源图像的宽度
            HEIGHT,             // 源图像的高度
            AV_PIX_FMT_YUYV422, // 源图像的像素格式
            WIDTH,              // 目标图像的宽度
            HEIGHT,             // 目标图像的高度
            AV_PIX_FMT_YUV420P, // 目标图像的像素格式
            SWS_BILINEAR,       // 选择缩放算法，例如双线性缩放
            NULL,               // 源过滤器，默认为 NULL
            NULL,               // 目标过滤器，默认为 NULL
            NULL                // 参数，默认为 NULL
    );

    if (!(*sws_ctx)) {
        // 错误处理: 创建 SwsContext 失败
        printf("Create SwsContext Failed!\n");
        exit(-1);
    }

    return;
}

//动态分配内存
/**
 * @brief 分配数据转换的空间
 * 
 * @param src_data 
 * @param src_linesize 
 * @param dst_data 
 * @param dst_linesize 
 */
void alloc_image_data(uint8_t **src_data, int *src_linesize, uint8_t **dst_data, int *dst_linesize){
    int ret = 0;

    ret = av_image_alloc(src_data, 
                        src_linesize, 
                        WIDTH, 
                        HEIGHT, 
                        AV_PIX_FMT_YUYV422, 
                        32);
    if(ret < 0){
        printf("Image src memory allocate Failed!\n");
        exit(-1);
    }

    ret = av_image_alloc(dst_data, 
                        dst_linesize, 
                        WIDTH, 
                        HEIGHT, 
                        AV_PIX_FMT_YUV420P, 
                        32);
                
    if(ret < 0){
        printf("Image dst memory allocate Failed!\n");
        exit(-1);
    }

    printf("src_linesize[0]: %d\n", src_linesize[0]);
    printf("dst_linesize[0]: %d\n", dst_linesize[0]);
    printf("dst_linesize[1]: %d\n", dst_linesize[1]);
    printf("dst_linesize[2]: %d\n", dst_linesize[2]);


    printf("============\n");
}

/**
 * @brief 释放转换的数据空间
 * 
 * @param src_data 
 * @param dst_data 
 */
void free_image_data(uint8_t **src_data, uint8_t **dst_data){
    /*
    *释放分配的内存，因为 src_data 是在栈上分配的，而数组里面的指针指向的才是在堆中分配的地址，
    所以需要用 av_freep(&src_data[0]),而不是 av_freep(src_data)
    */ 
    av_freep(&src_data[0]);
    av_freep(&dst_data[0]);
}

/**
 * @brief 图像数据格式转换
 * 
 * @param sws_ctx 
 * @param src_data 
 * @param src_linesize 
 * @param dst_data 
 * @param dst_linesize 
 */
void trans_format(struct SwsContext** sws_ctx, uint8_t **src_data, int *src_linesize, uint8_t **dst_data, int *dst_linesize){
    // 使用 sws_scale() 函数转换图像
    sws_scale(
        (*sws_ctx),           // 使用的 SwsContext
        (const uint8_t * const *)src_data, // 源图像数据
        src_linesize,      // 源图像每行的大小
        0,                 // 源图像的开始扫描线
        HEIGHT,            // 源图像的扫描线数量
        dst_data,          // 目标图像数据
        dst_linesize       // 目标图像每行的大小
    );

    return;
}

/**
 * @brief frame送到编码器，编码
 * 
 * @param enc_ctx 
 * @param frame 
 * @param newpkt 
 * @param outfile 
 */
void encode_video(AVCodecContext *enc_ctx, AVFrame *frame, AVPacket *newpkt, FILE *outfile){
    if(!enc_ctx){
        printf("Video Encoder Is Not Open!\n");
        exit(-1);
    }

    if(frame){
        printf("send frame to encoder, pts=%lld\n", frame->pts);
    }

    int ret = 0;
    ret = avcodec_send_frame(enc_ctx, frame);
    if(ret < 0){
        printf("Failed tp send a frame for encoding!\n");
        exit(-1);
    }

    //有可能会收到很多 frame 之后才开始编码
    while (ret >= 0)
    {
        ret = avcodec_receive_packet(enc_ctx, newpkt);
        if(ret == AVERROR(EAGAIN) || ret == AVERROR_EOF){
            return;
        }
        else if(ret < 0){
            printf("Failed to encode!\n");
            exit(-1);
        }

        fwrite(newpkt->data, 1, newpkt->size, outfile);
        fflush(outfile);

        av_packet_unref(newpkt);
    }
    
}


void h264_encode(){
    //设置分辨率
    int width = 1280;
    int height = 720;
    printf("=====================\n");

    //打开设备，获取 format 上下文
    AVFormatContext* fmt_ctx = open_dev();

    //编码器上下文
    AVCodecContext *enc_ctx = NULL;
    open_encoder(&enc_ctx, width, height);

    //创建 AVFrame
    AVFrame *frame = create_frame(width, height);
    int base = 0;
    //创建 AVPacket
    AVPacket *newpkt = create_packet();

    //创建文件
    const char *yuvfilename = "/home/vaesong/Data/Workspace/Audio_Video/1-Introduction/h264/video.yuv";
    const char *filename = "/home/vaesong/Data/Workspace/Audio_Video/1-Introduction/h264/video.h264";
    FILE *yuvfile = fopen(yuvfilename, "wb+");
    FILE *outfile = fopen(filename, "wb+");


    //初始化数据包,以及相关参数
    AVPacket pkt;
    av_init_packet(&pkt);
    int ret = 0;
    int count = 0;

    //数据转换，从 yuyv422 到 yuv420p
    struct SwsContext* sws_ctx = NULL;
    //分配图像数据的源空间和目标空间
    uint8_t *src_data[4];
    int src_linesize[4];
    uint8_t *dst_data[4];
    int dst_linesize[4];

    init_sws(&sws_ctx);
    alloc_image_data(src_data, src_linesize, dst_data, dst_linesize);    
    

    while ((ret = av_read_frame(fmt_ctx, &pkt)) >= 0 && count++ < 100)
    {
        av_log(NULL, AV_LOG_INFO, "packet size is %d(%p)\n", pkt.size, pkt.data);

        //首先把 Packet 里面的数据拷贝到 src_data
        if (pkt.size == src_linesize[0] * height) {
            memcpy((void*)src_data[0], pkt.data, pkt.size);
        } else {
            // 错误处理：大小不匹配
            printf("Video Image Size and Src_data Size is not match!\n");
            exit(-1);
        }

        //YUYV YUYV     YUYV422
        //YYYYYYYYUUVV  YUV420p
        trans_format(&sws_ctx, src_data, src_linesize, dst_data, dst_linesize);

        //然后把数据放到 Frame 中，进行后续编码
        memcpy(frame->data[0], dst_data[0], dst_linesize[0] * HEIGHT);
        memcpy(frame->data[1], dst_data[1], dst_linesize[1] * HEIGHT / 2);
        memcpy(frame->data[2], dst_data[2], dst_linesize[2] * HEIGHT / 2);

        //转换完成之后，需要进行编码
        frame->pts = base++;
        encode_video(enc_ctx, frame, newpkt, outfile);

        
        
        // /* 对于 1280 * 720，yuv420p
        // 这里对Y，U，V 分别写入文件，另外这里的行大小是分配给每个分量的字节数
        // 对于 Y 分量，每一个像素都会对应一个字节，但是对于 U 和 V 来说，相当于行和高各采样 1/2
        // 所以从行的角度来看，U，V 已经变成了 640。因此最后对高进行 /2 就行了
        // */
        // fwrite(dst_data[0], 1, dst_linesize[0] * HEIGHT, outfile); // Y平面
        // fwrite(dst_data[1], 1, dst_linesize[1] * HEIGHT / 2, outfile); // U平面
        // fwrite(dst_data[2], 1, dst_linesize[2] * HEIGHT / 2, outfile); // V平面
        // fflush(outfile);

        //释放 pkt 数据描述符？
        av_packet_unref(&pkt);
    }

    //最后输入一个 NULL，防止丢帧    
    encode_video(enc_ctx, NULL, newpkt, outfile);


    if(yuvfile){
        fclose(yuvfile);
    }
    if(outfile){
        fclose(outfile);
    }

    if(frame){
        av_frame_free(&frame);
    }
    
    if(newpkt){
        av_packet_free(&newpkt);
    }
    if(enc_ctx){
        avcodec_free_context(&enc_ctx);
    }

    // 清理
    sws_freeContext(sws_ctx);
    free_image_data(src_data, dst_data);

}

int main(){

    h264_encode();

    return 0;
}
```



# RTMP

RTMP协议是Real Time Message Protocol（实时信息传输协议）的缩写，它是由Adobe公司提出的一种应用层的协议，用来解决多媒体数据传输流的多路复用（Multiplexing）和分包（packetizing）的问题

RTMP协议是要靠底层可靠的传输层协议（通常是TCP）来保证信息传输的可靠性的，默认使用端口**1935**。在基于传输层协议的链接建立完成后，RTMP协议也要客户端和服务器通过**“握手”**来建立基于传输层链接之上的**RTMP Connection连接**。RTMP Connection成功后会传输一些控制信息，如CreateStream命令会创建一个Stream链接，用于传输具体的音视频数据和控制这些信息传输的命令信息。

RTMP协议中**基本的数据单元称为消息（Message）**，即封装、解封装都是以Message为单位进行操作。当RTMP协议在互联网中传输数据的时候，为了更好地实现多路复用、分包和信息的公平性，发送端会把Message划分为带有**Message ID的Chunk**，每个Chunk可能是一个单独的Message，也可能是Message的一部分，在接受端会根据chunk中包含的data的长度，message id和message的长度把chunk还原成完整的Message，从而实现信息的收发。

多路复用，RTMP可以将来自不同视频流的切片（chunk）在单个连接上传输，这种方法被称为“多路复用”



## 创建基本流程

[RTMP协议详解](https://blog.csdn.net/weixin_39399492/article/details/128069969)

### socket 建立 TCP 连接



### RTMP 握手（Handshake）

先进行TCP握手后再进行RTMP握手。由三个固定长度的块组成，有简单握手和复杂握手两种方式，两种握手方式信息流转的过程是相同的，只是消息中携带的信息不同

握手实质上起到的是验证的作用，其中一项是会校验服务器，客户端的rtmp版本，如果版本兼容则可以收发数据，如果版本不兼容则说明不能收发数据，则握手会失败

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231112201005.png" style="zoom:50%;" />

简单握手中S2是C1的复制，C2是S1的复制

- C0 和 S0，都是 8 个 bit ，代表各自可以接受的 RTMP 版本
- C1 和 S1，长度都是 1536 字节。

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231112201352.png" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/vaesong/Images/master/20231112201543.png" style="zoom:67%;" />

- C2 和 S2，长度都是 1536 字节。基本就是 S1 和 C1 的副本。S2是C1的复制。 C2是S1的复制



### RTMP消息格式

消息（Message）是RTMP协议中基本的数据单元。由Message Header和Message Payload（可以理解成message body）组成

对于音视频数据而言每一个message就是一帧数据。对于flv的tag而言，就是对应rtmp每个message，一个tag就是一个message，是一一对应的关系；相当于每一个tag都封装成一个message。message payload的数据格式和tag data的数据格式是相同的，message header和tag header的格式不同

![](https://raw.githubusercontent.com/vaesong/Images/master/20231112203826.png)

![](https://raw.githubusercontent.com/vaesong/Images/master/20231112200043.png)

![](https://raw.githubusercontent.com/vaesong/Images/master/20231112202535.png)



### CHUNK

RTMP以Message为基本单位，通过把Message拆分成Chunk来进行网络发送。chunk data默认是128字节。chunk是RTMP最小的传输单元。目的是：防止一个大的数据包传输时间过长，阻塞其它数据包的传输。chunk合成message：接收端将接收到chunk的chunk data的大小加和，如果等于message payload（通过chunk->message header->message length获取）的则认为是同一个message

Chunk在传输时：同一个Message产生的多个Chunk只会串行发送。先发送的Chunk一定先到达。不同Message产生的Chunk可以并行发送。并行发送的Chunk复用了一条TCP链接

![](https://raw.githubusercontent.com/vaesong/Images/master/20231112202703.png)



多种chunk type的目的是：减少重复数据发送，提高chunk data的占比



  



### 建立 RTMP 连接





### 创建 RTMP 流



# FLV

![](https://raw.githubusercontent.com/vaesong/Images/master/20231113085552.png)



#   推流实战

基本流程：

1. 解析 FLV 文件
2. 获取音视频数据
3. 利用 librtmp 进行推流







![](https://raw.githubusercontent.com/vaesong/Images/master/20231113142325.png)





# 实战编程

## 多媒体文件

其实是一种容器，存放个 meta info 和各种流

![](https://raw.githubusercontent.com/vaesong/Images/master/20231115200046.png)

![](https://raw.githubusercontent.com/vaesong/Images/master/20231115200350.png)

## 操作目录

- 操作目录函数

```C
avio_open_dir()
    
avio_read_dir()
    
avio_close_dir()
```





- 相关结构体

```C
// 操作目录的上下文
AVIODirContext
    
//目录项，用于存放文件名，文件大小等信息
AVIODirEntry
```



## 从多媒体文件中抽取音频

1. 打开多媒体文件
2. 从多媒体文件中找到流
3. 设置目的文件的上下文
4. 为多媒体文件创建新的音频流
5. 设置输出音频编码参数
6. 把输出上下文和输出文件绑定
7. 把多媒体文件头写入到目的文件
8. 从多媒体文件中读取音频数据到目的文件
9. 写多媒体文件尾到文件中

```C
#include <stdio.h>
#include <libavformat/avformat.h>
#include <libavdevice/avdevice.h>
#include <libavutil/avutil.h>

int main(int argc, char *argv[]){

    av_log_set_level(AV_LOG_INFO);

    avdevice_register_all();
    char *input = "../bilibili.mp4";
    char *output = "../extra_audio.aac";

    AVFormatContext *in_fmt_ctx = NULL;
    int ret = 0;

    //打开多媒体文件
    ret = avformat_open_input(&in_fmt_ctx, input, NULL, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Can't open file: %s\n", av_err2str(ret));
        exit(-1);
    }
    
    //从多媒体文件中找到流
    int audio_index = av_find_best_stream(in_fmt_ctx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Can not find the best stream!\n");
        goto _ERROR;
    }

    //设置目的文件的上下文
    AVFormatContext *out_fmt_ctx = NULL;
    avformat_alloc_output_context2(&out_fmt_ctx, NULL, NULL, output);
    if(!out_fmt_ctx){
        av_log(NULL, AV_LOG_ERROR, "Failed to allocate for out_fmt_ctx!\n");
        goto _ERROR;
    }

    //为多媒体文件创建新的音频流
    AVStream * out_stream = avformat_new_stream(out_fmt_ctx, NULL);

    //设置输出音频参数
    AVStream *in_stream = in_fmt_ctx->streams[audio_index];
    avcodec_parameters_copy(out_stream->codecpar, in_stream->codecpar);
    out_stream->codecpar->codec_tag = 0;

    //把输出上下文和输出文件绑定
    ret = avio_open2(&out_fmt_ctx->pb, output, AVIO_FLAG_WRITE, NULL, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Bind error!\n");
        goto _ERROR;
    }

    //把多媒体文件头写入到目的文件
    ret = avformat_write_header(out_fmt_ctx, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Writer header error!\n");
        goto _ERROR;
    }

    //从多媒体文件中读取音频数据到目的文件
    AVPacket pkt;
    while (av_read_frame(in_fmt_ctx, &pkt) >= 0)
    {
        if(pkt.stream_index == audio_index){
            //设置 Packet 的参数
            pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AV_ROUND_INF | AV_ROUND_PASS_MINMAX));
            pkt.dts = pkt.pts;
            pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
            pkt.stream_index = 0;
            pkt.pos = -1;
            av_interleaved_write_frame(out_fmt_ctx, &pkt);
            av_packet_unref(&pkt);
        }
    }

    // av_dump_format(in_fmt_ctx, 0, input, 0);
    // avformat_close_input(&in_fmt_ctx);

    //写多媒体文件尾到文件中
    av_write_trailer(out_fmt_ctx);

_ERROR:
    if(in_fmt_ctx){
        avformat_close_input(&in_fmt_ctx);
        exit(-1);
    }
    if(out_fmt_ctx->pb){
        avio_close(out_fmt_ctx->pb);
    }
    if(out_fmt_ctx){
        avformat_close_input(&out_fmt_ctx);
        exit(-1);
    }

    return 0;

}
```



## 修改多媒体文件封装格式

也是从源多媒体文件中抽出来音频和视频以及字幕数据包

然后注意**多路流**的映射，以及各个流之间的数据包的参数保持一致

```C
设置目标文件上下文有变化
avformat_alloc_output_context2(&out_fmt_ctx, NULL, NULL, output);

修改 Packet 数据包的时间戳的方式变化
av_packet_rescale_ts(&pkt, in_stream->time_base, out_stream->time_base);
```



```C
#include <stdio.h>
#include <libavformat/avformat.h>
#include <libavdevice/avdevice.h>
#include <libavutil/avutil.h>

int main(int argc, char *argv[]){

    av_log_set_level(AV_LOG_INFO);

    avdevice_register_all();
    char *input = "../bilibili.mp4";
    char *output = "../remux.flv";

    AVFormatContext *in_fmt_ctx = NULL;
    int ret = 0;

    //打开多媒体文件
    ret = avformat_open_input(&in_fmt_ctx, input, NULL, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Can't open file: %s\n", av_err2str(ret));
        exit(-1);
    }
    
    //从多媒体文件中找到流
    int audio_index = av_find_best_stream(in_fmt_ctx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Can not find the best stream!\n");
        goto _ERROR;
    }

    //设置目的文件的上下文
    AVFormatContext *out_fmt_ctx = NULL;
    avformat_alloc_output_context2(&out_fmt_ctx, NULL, NULL, output);
    if(!out_fmt_ctx){
        av_log(NULL, AV_LOG_ERROR, "Failed to allocate for out_fmt_ctx!\n");
        goto _ERROR;
    }

    //创建音频流、视频流、字幕流
    int *stream_map = NULL;
    stream_map = (int *)av_calloc(in_fmt_ctx->nb_streams, sizeof(int));
    if(!stream_map){
        av_log(NULL, AV_LOG_ERROR, "No memory to allocate stram map!\n");
        goto _ERROR;
    }

    int stream_index = 0;
    for(int i = 0; i < in_fmt_ctx->nb_streams; ++i){
        AVStream *in_stream = in_fmt_ctx->streams[i];
        AVCodecParameters *in_codec_par = in_stream->codecpar;
        AVStream *out_stream = NULL;

        //如果该流不属于音频，视频，字幕
        if(in_codec_par->codec_type != AVMEDIA_TYPE_AUDIO &&
           in_codec_par->codec_type != AVMEDIA_TYPE_VIDEO &&
           in_codec_par->codec_type != AVMEDIA_TYPE_SUBTITLE){
            stream_map[i] = -1;
            continue;
        }
        stream_map[i] = stream_index++;

        out_stream = avformat_new_stream(out_fmt_ctx, NULL);
        if(!out_stream){
            av_log(NULL, AV_LOG_ERROR, "Create out stream Failed!!\n");
            goto _ERROR;
        }

        avcodec_parameters_copy(out_stream->codecpar, in_stream->codecpar);
        out_stream->codecpar->codec_tag = 0;
    }

    //把输出上下文和输出文件绑定
    ret = avio_open2(&out_fmt_ctx->pb, output, AVIO_FLAG_WRITE, NULL, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Bind error!\n");
        goto _ERROR;
    }

    //把多媒体文件头写入到目的文件
    ret = avformat_write_header(out_fmt_ctx, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Writer header error!\n");
        goto _ERROR;
    }

    //从多媒体文件中读取音频数据到目的文件
    AVPacket pkt;
    while (av_read_frame(in_fmt_ctx, &pkt) >= 0)
    {
        AVStream *in_stream = NULL;
        AVStream *out_stream = NULL;

        
        if(stream_map[pkt.stream_index] == -1){
            av_packet_unref(&pkt);
            continue;
        }
        
        //查找经过映射后的对应的 流
        in_stream = in_fmt_ctx->streams[pkt.stream_index];
        pkt.stream_index = stream_map[pkt.stream_index];
        out_stream = out_fmt_ctx->streams[pkt.stream_index];

        av_packet_rescale_ts(&pkt, in_stream->time_base, out_stream->time_base);
        pkt.pos = -1;
        av_interleaved_write_frame(out_fmt_ctx, &pkt);

        av_packet_unref(&pkt);
    }

    // av_dump_format(in_fmt_ctx, 0, input, 0);
    // avformat_close_input(&in_fmt_ctx);

    //写多媒体文件尾到文件中
    av_write_trailer(out_fmt_ctx);

_ERROR:
    if(in_fmt_ctx){
        avformat_close_input(&in_fmt_ctx);
        exit(-1);
    }
    if(out_fmt_ctx->pb){
        avio_close(out_fmt_ctx->pb);
    }
    if(out_fmt_ctx){
        avformat_close_input(&out_fmt_ctx);
        exit(-1);
    }
    if(stream_map){
        av_free(stream_map);
    }
    return 0;

}
```



## 多媒体文件裁剪

![](https://raw.githubusercontent.com/vaesong/Images/master/20231115212523.png)

注意时间戳

```C
#include <stdio.h>
#include <stdlib.h>
#include <libavformat/avformat.h>
#include <libavdevice/avdevice.h>
#include <libavutil/avutil.h>

int main(int argc, char *argv[]){

    av_log_set_level(AV_LOG_INFO);
    if(argc != 5){
        av_log(NULL, AV_LOG_ERROR, "Parameters should be <./cut> <input_file> <out_file> <start_time> <end_time>!\n");
        exit(-1);
    }

    
    char *input = argv[1];
    char *output = argv[2];
    double start_time = atof(argv[3]); 
    double end_time = atof(argv[4]); 

    avdevice_register_all();
    AVFormatContext *in_fmt_ctx = NULL;
    int ret = 0;

    //打开多媒体文件
    ret = avformat_open_input(&in_fmt_ctx, input, NULL, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Can't open file: %s\n", av_err2str(ret));
        exit(-1);
    }
    
    //从多媒体文件中找到流
    int audio_index = av_find_best_stream(in_fmt_ctx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Can not find the best stream!\n");
        goto _ERROR;
    }

    //设置目的文件的上下文
    AVFormatContext *out_fmt_ctx = NULL;
    avformat_alloc_output_context2(&out_fmt_ctx, NULL, NULL, output);
    if(!out_fmt_ctx){
        av_log(NULL, AV_LOG_ERROR, "Failed to allocate for out_fmt_ctx!\n");
        goto _ERROR;
    }

    //创建音频流、视频流、字幕流
    int *stream_map = NULL;
    stream_map = (int *)av_calloc(in_fmt_ctx->nb_streams, sizeof(int));
    if(!stream_map){
        av_log(NULL, AV_LOG_ERROR, "No memory to allocate stram map!\n");
        goto _ERROR;
    }

    int stream_index = 0;
    for(int i = 0; i < in_fmt_ctx->nb_streams; ++i){
        AVStream *in_stream = in_fmt_ctx->streams[i];
        AVCodecParameters *in_codec_par = in_stream->codecpar;
        AVStream *out_stream = NULL;

        //如果该流不属于音频，视频，字幕
        if(in_codec_par->codec_type != AVMEDIA_TYPE_AUDIO &&
           in_codec_par->codec_type != AVMEDIA_TYPE_VIDEO &&
           in_codec_par->codec_type != AVMEDIA_TYPE_SUBTITLE){
            stream_map[i] = -1;
            continue;
        }
        stream_map[i] = stream_index++;

        out_stream = avformat_new_stream(out_fmt_ctx, NULL);
        if(!out_stream){
            av_log(NULL, AV_LOG_ERROR, "Create out stream Failed!!\n");
            goto _ERROR;
        }

        avcodec_parameters_copy(out_stream->codecpar, in_stream->codecpar);
        out_stream->codecpar->codec_tag = 0;
    }

    //把输出上下文和输出文件绑定
    ret = avio_open2(&out_fmt_ctx->pb, output, AVIO_FLAG_WRITE, NULL, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Bind error!\n");
        goto _ERROR;
    }

    //把多媒体文件头写入到目的文件
    ret = avformat_write_header(out_fmt_ctx, NULL);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "Writer header error!\n");
        goto _ERROR;
    }

    //查找开始时间附近的 I 帧，跳转到起始位置
    ret = av_seek_frame(in_fmt_ctx, -1, AV_TIME_BASE*start_time, AVSEEK_FLAG_BACKWARD);
    if(ret < 0){
        av_log(NULL, AV_LOG_ERROR, "%s", av_err2str(ret));
        goto _ERROR;
    }
    //数组映射，每个流的起始时间戳,初始化为 -1
    int64_t *dts_start_time = av_calloc(in_fmt_ctx->nb_streams, sizeof(int64_t));
    int64_t *pts_start_time = av_calloc(in_fmt_ctx->nb_streams, sizeof(int64_t));
    for(int t = 0; t < in_fmt_ctx->nb_streams; t++){
        dts_start_time[t] = -1;
        pts_start_time[t] = -1;
    }


    //从多媒体文件中读取音频数据到目的文件
    AVPacket pkt;
    while (av_read_frame(in_fmt_ctx, &pkt) >= 0)
    {
        AVStream *in_stream = NULL;
        AVStream *out_stream = NULL;

        //如果是该流的第一个数据包，那么就是开始的时间戳
        if(dts_start_time[pkt.stream_index] == -1 && pkt.dts > 0){
            dts_start_time[pkt.stream_index] = pkt.dts;
        }
        if(pts_start_time[pkt.stream_index] == -1 && pkt.pts > 0){
            pts_start_time[pkt.stream_index] = pkt.pts;
        }
        
        if(stream_map[pkt.stream_index] == -1){
            av_packet_unref(&pkt);
            continue;
        }
        
        //查找经过映射后的对应的 流
        in_stream = in_fmt_ctx->streams[pkt.stream_index];

        //如果截取完成
        if(av_q2d(in_stream->time_base) * pkt.pts > end_time){
            av_log(NULL, AV_LOG_INFO, "Success!\n");
            break;
        }

        //设置时间戳，如果解码时间比显示晚，要修正
        pkt.dts = pkt.dts - dts_start_time[pkt.stream_index];
        pkt.pts = pkt.pts - pts_start_time[pkt.stream_index];
        if(pkt.dts > pkt.pts){
            pkt.pts = pkt.dts;
        }
    

        //流的映射
        pkt.stream_index = stream_map[pkt.stream_index];
        out_stream = out_fmt_ctx->streams[pkt.stream_index];


        //设置时间基，
        av_packet_rescale_ts(&pkt, in_stream->time_base, out_stream->time_base);
        pkt.pos = -1;
        av_interleaved_write_frame(out_fmt_ctx, &pkt);

        av_packet_unref(&pkt);
    }

    // av_dump_format(in_fmt_ctx, 0, input, 0);
    // avformat_close_input(&in_fmt_ctx);

    //写多媒体文件尾到文件中
    av_write_trailer(out_fmt_ctx);

_ERROR:
    if(in_fmt_ctx){
        avformat_close_input(&in_fmt_ctx);
        exit(-1);
    }
    if(out_fmt_ctx->pb){
        avio_close(out_fmt_ctx->pb);
    }
    if(out_fmt_ctx){
        avformat_close_input(&out_fmt_ctx);
        exit(-1);
    }
    if(stream_map){
        av_free(stream_map);
    }
    if(dts_start_time){
        av_free(dts_start_time);
    }
    if(pts_start_time){
        av_free(pts_start_time);
    }
    return 0;

}
```



## SDL（Simple DirectMedia Layer）

[源码下载编译](https://libsdl.org/)

```Shell
./configure --prefix=/usr/local

sudo make -j 4 && sudo make install
```



## SDL 渲染窗口

创建销毁

事件种类

```C
SDL_WindowEvent: 窗口事件
SDL_KeyboardEvent: 键盘事件
SDL_MouseMotionEvent: 鼠标事件
```



```C
#include <stdio.h>
#include <SDL2/SDL.h>

int main(){

    //初始化
    SDL_Init(SDL_INIT_VIDEO);

    //内存中创建窗口
    SDL_Window * window = SDL_CreateWindow("SDL2 Window",
                                            600,
                                            600,
                                            640,
                                            480,
                                            SDL_WINDOW_SHOWN);

    if(!window){
        printf("Failed to create window!\n");
        goto _ERROR;
    }

    //需要给到 GPU 去渲染
    SDL_Renderer *render = SDL_CreateRenderer(window, -1, 0);
    if(!render){
        SDL_Log("Failed to create render!\n");
    }

    SDL_bool done = SDL_FALSE;
    while (!done) {
        SDL_Event event;
        while (SDL_PollEvent(&event)) {
            if (event.type == SDL_QUIT) {
                done = SDL_TRUE;
            }
        }

        SDL_SetRenderDrawColor(render, 0, 255, 0, 255);
        SDL_RenderClear(render);
        
        // 在这里添加绘图代码

        SDL_RenderPresent(render);
        SDL_Delay(16); // 约60帧每秒
    }

    // //设置颜色
    // SDL_SetRenderDrawColor(render, 0, 255, 0, 255);

    // //清屏
    // SDL_RenderClear(render);

    // //把窗口推到render
    // SDL_RenderPresent(render);

    // SDL_Delay(20000);
    

_ERROR:
    if(window){
        SDL_DestroyWindow(window);
    }
    if(render){
        SDL_DestroyRenderer(render);
    }
    //退出
    SDL_Quit();

    return 0;
}
```



## 纹理

图像需要渲染成纹理再去显示，减少了空间？

大概就是开辟一个纹理缓冲区，告诉 GPU 应该怎么画，GPU很强，能够仅仅根据描述信息就画出来了，之后，把这个纹理渲染到窗口上（把画好的贴上去）

![](https://raw.githubusercontent.com/vaesong/Images/master/20231116103056.png)

纹理渲染

纹理渲染就是将缓冲区（内存）中保存的图像描述的信息（纹理）生成图像的过程（渲染）。
案例理解： 小A同学画一幅画

下面，我们以生活中的一个例子‘小A同学画一幅画’ 来解释SDL渲染，纹理概念。

计算机使用SDL渲染显示一幅图就相当于小A同学在墙上画一幅画， 我们先将整个过程中的角色划分为纹理， 渲染器， gpu(画家)， cpu(小A同学)

依据SDL编程方式，这里又分为两种情况：

    不使用纹理
    
    即由cpu直接绘制一幅画(cpu需要将最原始的rgb/YUV数据，刷到屏幕上)， 相当于学生小A直接在墙上画画
    
    使用纹理
    
    相当于小A同学(cpu)指挥画家(gpu)在纸上画， 然后把纸贴在墙上。 这个过程中画是由画家(gpu)画的， 小A同学负责发号施令(即告诉画家画什么)， 纸代表纹理， 画家代表gpu， 所有绘制的操作都是在纹理上进行。事实上，纹理的概念并不仅仅是一张纸， 还包括小A同学中对这幅画的构思，可以理解成画画的算法， 而纸相当于是一个载体(内存空间，用于保存这些构思)。 gpu根据纹理就可以计算出这幅图每个像素点的颜色( 相当于画家根据小A同学的描述，画出一幅画一样)


```C
SDL_CreateTexture()
format: YUV,RGB
access: Texture类型, Target, Stream
    
SDL_DestoryTexture()
```



```C
//渲染的目标
ADL_SetRenderTarget()
    
//清屏，用设置的颜色
SDL_RenderClear()

//纹理拷贝到显卡上
SDL_RenderCopy()
    
//投影到显示器上
SDL_RenderPresent()
```



生成一个随机跳动的小方块

```C
#include <stdio.h>
#include <SDL2/SDL.h>

int main(){

    //创建一个小方块
    SDL_Rect rect;
    rect.w = 30;
    rect.h = 30;

    //初始化
    SDL_Init(SDL_INIT_VIDEO);

    //内存中创建窗口
    SDL_Window * window = SDL_CreateWindow("SDL2 Window",
                                            600,
                                            600,
                                            640,
                                            480,
                                            SDL_WINDOW_SHOWN);

    if(!window){
        printf("Failed to create window!\n");
        goto _ERROR;
    }

    //需要给到 GPU 去渲染
    SDL_Renderer *render = SDL_CreateRenderer(window, -1, 0);
    if(!render){
        SDL_Log("Failed to create render!\n");
    }
    
    //创建纹理
    SDL_Texture *texture = SDL_CreateTexture(render, SDL_PIXELFORMAT_BGRA8888, SDL_TEXTUREACCESS_TARGET, 600, 480);
    if(!texture){
        SDL_Log("Failed to create texture!\n");
        goto _ERROR;
    }


    SDL_bool done = SDL_FALSE;
    while (!done) {
        SDL_Event event;
        while (SDL_PollEvent(&event)) {
            if (event.type == SDL_QUIT) {
                done = SDL_TRUE;
            }
        }

        // SDL_SetRenderDrawColor(render, 0, 255, 0, 255);
        // SDL_RenderClear(render);
        
        // 在这里添加绘图代码
        rect.x = rand() % 600;
        rect.y = rand() % 480;

        //刷新纹理，填充颜色
        SDL_SetRenderTarget(render, texture);
        SDL_SetRenderDrawColor(render, 0, 0, 0, 0);
        SDL_RenderClear(render);

        //绘制方块
        SDL_RenderDrawRect(render, &rect);
        //填充颜色
        SDL_SetRenderDrawColor(render, 0, 0, 255, 0);
        SDL_RenderFillRect(render, &rect);

        //然后重新渲染整个窗口
        SDL_SetRenderTarget(render, NULL);
        //把纹理拷贝到窗口
        SDL_RenderCopy(render, texture, NULL, NULL);
        //显示
        SDL_RenderPresent(render);

        //显示时间， 0.5s 变化一次
        SDL_Delay(500);
    }
    

_ERROR:
    if(window){
        SDL_DestroyWindow(window);
    }
    if(render){
        SDL_DestroyRenderer(render);
    }
    if(texture){
        SDL_DestroyTexture(texture);
    }
    //退出
    SDL_Quit();

    return 0;
}
```



# VPlayer

![](https://raw.githubusercontent.com/vaesong/Images/master/20231121095607.png)



## 视频

从文件中读取视频，然后设置相应的解码器，再通过渲染，展示出画面



## 音视频同步

- **视频同步到音频**：一般的做法，展示第一帧视频帧后，获得要显示的下一个视频帧的 PTS，然后设置一个定时器，当定时器超时后，刷新新的视频帧，如此反复操作
- 音频同步到视频
- 音频和视频都同步到系统时钟





## 架构

从线程角度看，总共分了四个线程：

- **主线程**：用于创建 SDL 窗口和渲染画面，监听 SDL 事件，初始化结构体参数等
- **线程1**：完成初始的任务，包括打开文件，找到相应的 音频、视频流，初始化结构体相应参数等。然后不停的读取数据，放到相应的 Packet 队列
- **线程2**：扬声器调用回调函数，读取音频队列中的数据，解码到 audio_buf 数组中，然后送往相应的位置
- **线程3**：用于视频数据的解码，以及渲染



